{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image captioning \n",
    "あんまりうまくいかなかった　　\n",
    "結局画像の特徴量を見つける必要があるので難しそう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\")\n",
    "\n",
    "img_url = '/root/signate_tecno/input/test/Zzxnb7ogjzruKrw29pNvk3.jpg' \n",
    "raw_image = Image.open(img_url).convert('RGB')\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"a photography of a human dancing holding\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "\"\"\"\n",
    "# unconditional image captioning\n",
    "inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connected componentsをチャンネルとして追加したい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset,Subset, random_split\n",
    "from sklearn.model_selection import KFold\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "import torch.nn as nn\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import lightning as L\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torchmetrics, argparse\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "\n",
    "import multiprocessing\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "print(num_workers)\n",
    "import timm\n",
    "import wandb\n",
    "import segmentation_models_pytorch as smp\n",
    "from ultralytics import YOLO\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像が保存されているディレクトリ\n",
    "input_dir = \"/root/signate_tecno/input/test/\"\n",
    "\n",
    "# 画像ファイルのリストを取得\n",
    "image_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.jpg')]\n",
    "\n",
    "output_dir = \"/root/signate_tecno/input/edge_test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像を読み込む\n",
    "for image_file in image_files:\n",
    "    # 画像を読み込む。\n",
    "    img = cv2.imread(image_file)\n",
    "\n",
    "    # グレースケールに変換する。\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 2値化する\n",
    "    ret, bin_img = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
    "    # connected componentsを検出\n",
    "    num_labels, labels_im = cv2.connectedComponents(bin_img)\n",
    "    # 出力ファイル名を作成\n",
    "    output_file = os.path.join(output_dir, os.path.basename(image_file))\n",
    "    cv2.imwrite(output_file, labels_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エッジ検出を行う関数を追加します\n",
    "\n",
    "def detect_edges(image_path):\n",
    "    # 画像を読み込む\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # エッジ検出を行う\n",
    "    edges = cv2.Canny(image, 100, 200)\n",
    "    \n",
    "    return edges\n",
    "\n",
    "for image_file in image_files:\n",
    "    edges = detect_edges(image_file)\n",
    "    output_file = os.path.join(output_dir, os.path.basename(image_file))\n",
    "    cv2.imwrite(output_file, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
