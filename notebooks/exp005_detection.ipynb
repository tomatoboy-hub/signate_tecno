{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset,Subset, random_split\n",
    "from sklearn.model_selection import KFold\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "import torch.nn as nn\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import lightning as L\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torchmetrics, argparse\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "\n",
    "import multiprocessing\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "print(num_workers)\n",
    "import timm\n",
    "import wandb\n",
    "import segmentation_models_pytorch as smp\n",
    "from ultralytics import YOLO\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像が保存されているディレクトリ\n",
    "input_dir = \"/root/signate_tecno/input/test/\"\n",
    "\n",
    "# 画像ファイルのリストを取得\n",
    "image_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.jpg')]\n",
    "\n",
    "output_dir = \"/root/signate_tecno/input/crop_test/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in image_files:\n",
    "    # 推論を実行\n",
    "    results = model.predict(image_path, conf=0.5)\n",
    "    detections = results[0]\n",
    "    \n",
    "    # 人物のバウンディングボックスを抽出\n",
    "    person_boxes = []\n",
    "    confidences = []\n",
    "    for result in detections.boxes:\n",
    "        class_id = int(result.cls.cpu().numpy())\n",
    "        if class_id == 0:  # 'person'\n",
    "            bbox = result.xyxy.cpu().numpy()[0]\n",
    "            confidence = result.conf.cpu().numpy()[0]\n",
    "            person_boxes.append(bbox)\n",
    "            confidences.append(confidence)\n",
    "    \n",
    "    # 画像を読み込み\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    if person_boxes:\n",
    "        # 最も信頼度の高いバウンディングボックスを選択\n",
    "        max_conf_idx = np.argmax(confidences)\n",
    "        bbox = person_boxes[max_conf_idx]\n",
    "        \n",
    "        # 余白を追加する（例：10ピクセル）\n",
    "        padding = 10\n",
    "        x_min, y_min, x_max, y_max = map(int, bbox)\n",
    "        height, width, _ = img.shape\n",
    "        x_min = max(0, x_min-padding)\n",
    "        y_min = 0\n",
    "        x_max = min(width, x_max+padding)\n",
    "        y_max = height\n",
    "        cropped_img = img[y_min:y_max, x_min:x_max]\n",
    "    else:\n",
    "        # 検出がなかった場合は元の画像を使用\n",
    "        cropped_img = img\n",
    "    \n",
    "    # 保存先のファイル名を作成\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_path = os.path.join(output_dir, f\"{base_name}.jpg\")\n",
    "    cv2.imwrite(output_path, cropped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in image_files:\n",
    "    # 推論を実行\n",
    "    results = model.predict(image_path, conf=0.5)\n",
    "    detections = results[0]\n",
    "    \n",
    "    # 人物のバウンディングボックスを抽出\n",
    "    person_boxes = []\n",
    "    for result in detections.boxes:\n",
    "        class_id = int(result.cls.cpu().numpy())\n",
    "        if class_id == 0:  # 'person'\n",
    "            bbox = result.xyxy.cpu().numpy()[0]\n",
    "            person_boxes.append(bbox)\n",
    "    \n",
    "    # 画像を読み込み\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # 各人物をクロップして保存\n",
    "    for idx, bbox in enumerate(person_boxes):\n",
    "        # 余白を追加する（例：10ピクセル）\n",
    "        padding = 10\n",
    "        x_min, y_min, x_max, y_max = map(int, bbox)\n",
    "        height, width, _ = img.shape\n",
    "        x_min = max(0, x_min-padding)\n",
    "        y_min = 0\n",
    "        x_max = min(width, x_max+padding)\n",
    "        y_max = height\n",
    "        cropped_img = img[y_min:y_max, x_min:x_max]\n",
    "        # 保存先のファイル名を作成\n",
    "        base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        output_path = os.path.join(output_dir, f\"{base_name}.jpg\")\n",
    "        cv2.imwrite(output_path, cropped_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langsamによるtextプロンプト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from lang_sam import LangSAM\n",
    "\n",
    "model = LangSAM()\n",
    "image_pil = Image.open(\"/root/signate_tecno/input/train/hold/2a2PfZsZkYfYTX77zPjYep.jpg\").convert(\"RGB\")\n",
    "text_prompt = \"fan\"\n",
    "masks, boxes, phrases, logits = model.predict(image_pil, text_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## samによる検出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import SAM\n",
    "\n",
    "# Load a model\n",
    "model = SAM(\"sam2_b.pt\")\n",
    "\n",
    "# Display model information (optional)\n",
    "model.info()\n",
    "\n",
    "# Run inference\n",
    "result = model(\"/root/signate_tecno/input/test/AKtCNcoN3x3a6Avwjjf2tw.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(\"/root/signate_tecno/input/test/AKtCNcoN3x3a6Avwjjf2tw.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 画像の読み込み\n",
    "image = cv2.imread(\"/root/signate_tecno/input/test/AKtCNcoN3x3a6Avwjjf2tw.jpg\")\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#マスクをNumPy配列に変換し、CPU上で操作\n",
    "masks = result[0].masks.data\n",
    "masks = masks.cpu().numpy()\n",
    "\n",
    "# マスクの数だけ色を用意\n",
    "num_masks = masks.shape[0]\n",
    "colors = plt.cm.get_cmap('hsv', num_masks)\n",
    "\n",
    "# 元の画像をコピーして表示用に準備\n",
    "overlay = image_rgb.copy()\n",
    "for i in range(num_masks):\n",
    "    # マスクを取得\n",
    "    mask = masks[i]\n",
    "    # マスクをブール型に変換\n",
    "    mask = mask.astype(bool)\n",
    "    # マスクの色を設定\n",
    "    color = np.array(colors(i))[:3] * 255  # 色をRGBの範囲にスケーリング\n",
    "    color = color.astype(np.uint8)\n",
    "    # マスクを重ねる\n",
    "    overlay[mask] = overlay[mask] * 0.5 + color * 0.5  # 元の色とマスク色をブレンド\n",
    "\n",
    "# 画像を表示\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(overlay)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
