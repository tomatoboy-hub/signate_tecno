{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3325,"status":"ok","timestamp":1728483756778,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"LVh0Z4vGk-uu","outputId":"851eb8b9-81c6-4b1c-fd82-a9210b1313fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                 file_name  \\\n","0      228dQDvEVcm4PXMP9bw2GW_person_0.jpg   \n","1      22CqhijAHeT8mBadFcdpio_person_0.jpg   \n","2      22FVaETtQmfDahcBsKxpTg_person_0.jpg   \n","3      22K2UAjYrJeqedaNjsCF7S_person_0.jpg   \n","4      22MnhXiaah9MvND96yYcRg_person_0.jpg   \n","...                                    ...   \n","32721  oYpo8XhNy4KBg2vRkMEBzg_person_0.jpg   \n","32722  oYzLvTPoZ5cPECMhm6juVe_person_0.jpg   \n","32723  oYzfB6USChTaUaMZCFGfS3_person_0.jpg   \n","32724  oZ24WQinzvEdPqCmgzqdbA_person_0.jpg   \n","32725  oZ2DYcm9y86z5nVseWYyu4_person_0.jpg   \n","\n","                                               file_path  \\\n","0      /root/signate_tecno/input/crop_train/not-hold/...   \n","1      /root/signate_tecno/input/crop_train/not-hold/...   \n","2      /root/signate_tecno/input/crop_train/not-hold/...   \n","3      /root/signate_tecno/input/crop_train/not-hold/...   \n","4      /root/signate_tecno/input/crop_train/not-hold/...   \n","...                                                  ...   \n","32721  /root/signate_tecno/input/crop_train/hold/oYpo...   \n","32722  /root/signate_tecno/input/crop_train/hold/oYzL...   \n","32723  /root/signate_tecno/input/crop_train/hold/oYzf...   \n","32724  /root/signate_tecno/input/crop_train/hold/oZ24...   \n","32725  /root/signate_tecno/input/crop_train/hold/oZ2D...   \n","\n","                      caption  label  \n","0      not-hold a folding fan      0  \n","1      not-hold a folding fan      0  \n","2      not-hold a folding fan      0  \n","3      not-hold a folding fan      0  \n","4      not-hold a folding fan      0  \n","...                       ...    ...  \n","32721      hold a folding fan      1  \n","32722      hold a folding fan      1  \n","32723      hold a folding fan      1  \n","32724      hold a folding fan      1  \n","32725      hold a folding fan      1  \n","\n","[64851 rows x 4 columns]\n"]}],"source":["import os\n","import shutil\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","classifications = [\"not-hold a folding fan\", \"hold a folding fan\"]\n","\n","def search_images(directory):\n","    # 対応する画像の拡張子を定義\n","    image_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n","\n","    # 画像ファイルのパスとファイル名をリストに格納\n","    image_files = []\n","\n","    for root, dirs, files in os.walk(directory):\n","        for file in files:\n","            if file.lower().endswith(image_extensions):\n","                file_path = os.path.join(root, file)\n","                image_files.append({\n","                    'file_name': file,\n","                    'file_path': file_path\n","                })\n","\n","    # pandas DataFrame に変換\n","    df = pd.DataFrame(image_files)\n","\n","    return df\n","\n","#HOME = Path(\"/content\")\n","#INPUTS = HOME / \"dataset\"  # input data\n","INPUTS = Path(\"/root/signate_tecno/input\")\n","TRAIN_IMAGEDIR0 = INPUTS / \"crop_train\" / \"not-hold\"\n","TRAIN_IMAGEDIR1 = INPUTS / \"crop_train\" / \"hold\"\n","\n","train_df = pd.DataFrame()\n","train_df0 = search_images(TRAIN_IMAGEDIR0)\n","train_df1 = search_images(TRAIN_IMAGEDIR1)\n","train_df0['caption'] = classifications[0]\n","train_df1['caption'] = classifications[1]\n","train_df0['label'] = 0\n","train_df1['label'] = 1\n","train_df = pd.concat([train_df0, train_df1], axis=0)\n","print(train_df)\n","train_df.to_csv('/root/signate_tecno/input/train.csv', index=None)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7698,"status":"ok","timestamp":1728483767832,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"Dz7q0RKxlCdY"},"outputs":[],"source":["import os, shutil\n","\n","#必要なライブラリのインポート\n","import re, gc, sys\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from glob import glob\n","\n","import warnings, random\n","import cv2\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","from sklearn.model_selection import StratifiedKFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim import lr_scheduler\n","\n","import torchvision\n","from torchvision import transforms\n","import torchvision.models as models\n","from torch.cuda.amp import GradScaler\n","\n","import timm\n","import yaml\n","from tqdm import tqdm\n","import time\n","import copy\n","from collections import defaultdict\n","\n","from colorama import Fore, Back, Style\n","b_ = Fore.BLUE\n","y_ = Fore.YELLOW\n","sr_ = Style.RESET_ALL"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1728483767833,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"uzZjaObUlE0F","outputId":"b25284d8-0731-4c22-98d8-eb4dce27b2d0"},"outputs":[{"data":{"text/plain":["{'DATA_DIR': '/root/signate_tecno/input/',\n"," 'OUT_DIR': '/root/signate_tecno/output',\n"," 'model_name': 'vit_l_16',\n"," 'image_size': (224, 224),\n"," 'timm_model_name': 'vit_base_patch16_224',\n"," 'pretrained': True,\n"," 'n_fold': 2,\n"," 'epochs': 2,\n"," 'criterion': 'CrossEntropy',\n"," 'is_blurry': False,\n"," 'train_batch_size': 5,\n"," 'test_batch_size': 15,\n"," 'seed': 2023,\n"," 'optimizer': 'AdamW',\n"," 'learning_rate': 1e-05,\n"," 'scheduler': 'CosineAnnealingLR',\n"," 'min_lr': 1e-06,\n"," 'T_max': 500,\n"," 'n_accumulate': 1,\n"," 'clip_grad_norm': 'None',\n"," 'apex': True,\n"," 'num_classes': 2,\n"," 'device': device(type='cuda', index=0)}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["ARGS = {\n","  'DATA_DIR': '/root/signate_tecno/input/',\n","  'OUT_DIR': '/root/signate_tecno/output',\n","  'model_name': 'vit_l_16',\n","  'image_size': (224, 224), # vit_l16\n","  #cpu(slow)\n","  #'train_batch_size': 4,\n","  #'test_batch_size': 8,\n","  #gpu\n","  #'train_batch_size': 28, # 32(x)\n","  #'test_batch_size': 56,\n","  #'n_fold': 2,\n","  #'epochs': 3,\n","  #'timm_model_name': 'resnet50',\n","  'timm_model_name': 'vit_base_patch16_224',\n","  #lb:0.8545424 ? :\n","  #'timm_model_name': 'vit_large_patch32_224_in21k',\n","  #down?\n","  #'timm_model_name': 'vit_huge_patch14_224_in21k',\n","  #colab free crash memory?.(batch=4)\n","  #'timm_model_name': 'vit_giant_patch14_224_clip_laion2b',\n","  #Only one class present in y_true. ROC AUC score is not defined in that case.\n","  #lb:0.8944379  : top, batchsize : 4, 'image_size': (448, 448),\n","  #'timm_model_name': 'eva02_large_patch14_448.mim_m38m_ft_in22k_in1k',\n","  'pretrained': True,\n","  'n_fold': 2, # 5\n","  'epochs': 2, # 8\n","  #'image_size': (448, 448), # eva02_large_patch14_448\n","  'criterion': 'CrossEntropy',\n","  #'is_blurry': True,\n","  'is_blurry': False,\n","  #'image_size': (336, 336),\n","  #GPU: 16GB\n","  'train_batch_size': 5, # 4, #1(ng?)\n","  'test_batch_size': 15, #x3?\n","  #GPU: 80GB?\n","  #'train_batch_size': 32, # 4, #1(ng?)\n","  #'test_batch_size': 96,\n","  #batchsize row is roc_auc calc ng,(batchsizeが小さいと、class data が片方のみ存在している状態になり roc_auc の計算ができない)\n","  'seed': 2023,\n","  'optimizer': 'AdamW',\n","  'learning_rate': 1e-05,\n","  'scheduler': 'CosineAnnealingLR', # CosineAnnealingWarmRestarts\n","  'min_lr': 1e-06,\n","  'T_max': 500,\n","  'n_accumulate': 1,\n","  'clip_grad_norm': 'None',\n","  'apex': True,\n","  'num_classes': 2,\n","  'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  }\n","ARGS"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1728483767833,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"aWqK502-lHv0"},"outputs":[],"source":["def get_logger(filename):\n","    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler2 = FileHandler(filename=f\"{filename}.log\")\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler2)\n","    return logger\n","\n","#再現性を出すために必要な関数となります\n","def worker_init_fn(worker_id):\n","    torch.manual_seed(worker_id)\n","    random.seed(worker_id)\n","    np.random.seed(worker_id)\n","    torch.cuda.manual_seed(worker_id)\n","    os.environ['PYTHONHASHSEED'] = str(worker_id)\n","\n","def set_seed(seed=42):\n","    '''Sets the seed of the entire notebook so results are the same every time we run.\n","    This is for REPRODUCIBILITY.'''\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    # When running on the CuDNN backend, two further options must be set\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Set a fixed value for the hash seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","\n","LOGGER = get_logger(ARGS['OUT_DIR']+'train')\n","set_seed(ARGS[\"seed\"])"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1728483767833,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"qHafV2PtlI_x","outputId":"99180516-c86b-45b9-ac66-0d9a4cc1e8ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["Folds created successfully\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>file_name</th>\n","      <th>file_path</th>\n","      <th>caption</th>\n","      <th>label</th>\n","      <th>kfold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>228dQDvEVcm4PXMP9bw2GW.jpg</td>\n","      <td>/root/signate_tecno/input/train/not-hold/228dQ...</td>\n","      <td>not-hold a folding fan</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>22CqhijAHeT8mBadFcdpio.jpg</td>\n","      <td>/root/signate_tecno/input/train/not-hold/22Cqh...</td>\n","      <td>not-hold a folding fan</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>22FVaETtQmfDahcBsKxpTg.jpg</td>\n","      <td>/root/signate_tecno/input/train/not-hold/22FVa...</td>\n","      <td>not-hold a folding fan</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>22K2UAjYrJeqedaNjsCF7S.jpg</td>\n","      <td>/root/signate_tecno/input/train/not-hold/22K2U...</td>\n","      <td>not-hold a folding fan</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>22MnhXiaah9MvND96yYcRg.jpg</td>\n","      <td>/root/signate_tecno/input/train/not-hold/22Mnh...</td>\n","      <td>not-hold a folding fan</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    file_name  \\\n","0  228dQDvEVcm4PXMP9bw2GW.jpg   \n","1  22CqhijAHeT8mBadFcdpio.jpg   \n","2  22FVaETtQmfDahcBsKxpTg.jpg   \n","3  22K2UAjYrJeqedaNjsCF7S.jpg   \n","4  22MnhXiaah9MvND96yYcRg.jpg   \n","\n","                                           file_path                 caption  \\\n","0  /root/signate_tecno/input/train/not-hold/228dQ...  not-hold a folding fan   \n","1  /root/signate_tecno/input/train/not-hold/22Cqh...  not-hold a folding fan   \n","2  /root/signate_tecno/input/train/not-hold/22FVa...  not-hold a folding fan   \n","3  /root/signate_tecno/input/train/not-hold/22K2U...  not-hold a folding fan   \n","4  /root/signate_tecno/input/train/not-hold/22Mnh...  not-hold a folding fan   \n","\n","   label  kfold  \n","0      0      0  \n","1      0      1  \n","2      0      0  \n","3      0      0  \n","4      0      0  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["def create_folds(data, num_splits, seed):\n","    data[\"kfold\"] = -1\n","\n","    mskf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=seed)\n","    labels = [\"label\"]\n","    data_labels = data[labels].values\n","\n","    for f, (t_, v_) in enumerate(mskf.split(data, data_labels)):\n","        data.loc[v_, \"kfold\"] = f\n","\n","    return data\n","\n","train = pd.read_csv(f\"{ARGS['DATA_DIR']}/train.csv\")\n","train = create_folds(train, num_splits=ARGS[\"n_fold\"], seed=ARGS[\"seed\"])\n","print(\"Folds created successfully\")\n","\n","train.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1728483767833,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"dedN5t8KlNLX"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, df, transform, data_type):\n","        self.df = df\n","        self.data_type = data_type\n","\n","        if self.data_type == \"train\":\n","            self.image_paths = df['file_path']\n","            self.labels = df['label']\n","        if self.data_type == \"test\":\n","            #self.image_paths = df[0]\n","            self.image_paths = df['image_path']\n","\n","        self.transform= transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index: int):\n","        image_path = self.image_paths[index]\n","\n","        if self.data_type == \"train\":\n","            #image = cv2.imread(f\"/content/train/{file_name}\")\n","            image = cv2.imread(f\"{image_path}\")\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            label = self.labels[index]\n","            label = torch.tensor(label, dtype=torch.long)\n","\n","            image = self.transform(image=image)[\"image\"]\n","            return image, label\n","\n","        if self.data_type == \"test\":\n","            if os.path.exists(f\"/root/signate_tecno/input/crop_test/{image_path}\"):\n","              image = cv2.imread(f\"/root/signate_tecno/input/crop_test/{image_path}\")\n","              image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","              image = self.transform(image=image)[\"image\"]\n","            else:\n","              print(\"not found \" + image_path)\n","              image = None\n","\n","            return image"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["import os\n","\n","def rename_files(directory):\n","    for root, dirs, files in os.walk(directory):\n","        for file in files:\n","            if '_person_0' in file:\n","                new_file = file.replace('_person_0', '')\n","                os.rename(os.path.join(root, file), os.path.join(root, new_file))\n","\n","TEST_IMAGEDIR = INPUTS / \"crop_test\"\n","rename_files(TEST_IMAGEDIR)"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728483768948,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"S2oGvt1jlO97"},"outputs":[],"source":["class CustomDataset2(Dataset):\n","    def __init__(self, df, transform, data_type, is_blurry):\n","        self.df = df\n","        self.data_type = data_type\n","\n","        if self.data_type == \"train\":\n","            if is_blurry:\n","                self.image_paths = df['file_path']\n","            else:\n","                self.image_paths = df['file_path']\n","            self.labels = df['label']\n","        if self.data_type == \"test\":\n","            #self.image_paths = df[0]\n","            self.image_paths = df['image_path']\n","\n","        self.transform= transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index: int):\n","        image_path = self.image_paths[index]\n","\n","        if self.data_type == \"train\":\n","            image = cv2.imread(f\"/content/train/{image_path}\")\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            label = self.labels[index]\n","            label = torch.tensor(label, dtype=torch.long)\n","\n","            image = self.transform(image=image)[\"image\"]\n","            return image, label\n","\n","        if self.data_type == \"test\":\n","            image = cv2.imread(f\"/content/test/{image_path}\")\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            image = self.transform(image=image)[\"image\"]\n","\n","            return image"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":932,"status":"ok","timestamp":1728483769879,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"e7QY4F7alQeZ"},"outputs":[],"source":["import albumentations as albu\n","from albumentations.pytorch import transforms as AT\n","\n","# Augumentation用\n","#CV : up, LB : down\n","image_transform_train = albu.Compose([\n","    albu.Resize(ARGS[\"image_size\"][0], ARGS[\"image_size\"][1]),\n","    albu.HorizontalFlip(p=0.5),\n","    albu.VerticalFlip(p=0.5),\n","    albu.RandomBrightnessContrast(p=0.3),\n","    albu.RandomGamma(gamma_limit=(85, 115), p=0.3),\n","    albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.10, rotate_limit=45, p=0.5),\n","    albu.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    AT.ToTensorV2()\n","])\n","\n","image_transform = albu.Compose([\n","    albu.Resize(ARGS[\"image_size\"][0], ARGS[\"image_size\"][1]),\n","    # albu.HorizontalFlip(p=0.5),\n","    # albu.VerticalFlip(p=0.5),\n","    # albu.RandomBrightnessContrast(p=0.3),\n","    # albu.RandomGamma(gamma_limit=(85, 115), p=0.3),\n","    # albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.10, rotate_limit=45, p=0.5),\n","    albu.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    AT.ToTensorV2()\n","])"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1728483769879,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"-r0OlLCilSTC"},"outputs":[],"source":["def train_one_epoch(model, optimizer, train_loader, device, epoch):\n","    model.train()\n","    dataset_size = 0\n","    running_loss = 0.0\n","    running_score = []\n","    running_score_y = []\n","    scaler = GradScaler(enabled=ARGS[\"apex\"])\n","\n","    train_loss = []\n","    bar = tqdm(enumerate(train_loader), total=len(train_loader))\n","    for step, (images, targets) in bar:\n","      images = images.to(device)\n","      targets = targets.to(device)\n","\n","      batch_size = targets.size(0)\n","      with torch.cuda.amp.autocast(enabled=ARGS[\"apex\"]):\n","          outputs = model(images)\n","          loss = criterion(ARGS, outputs, targets)\n","\n","      scaler.scale(loss).backward()\n","\n","      if ARGS[\"clip_grad_norm\"] != \"None\":\n","          grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), ARGS[\"clip_grad_norm\"])\n","      else:\n","          grad_norm = None\n","\n","      scaler.step(optimizer)\n","      scaler.update()\n","\n","      optimizer.zero_grad()\n","\n","      if scheduler is not None:\n","          scheduler.step()\n","\n","      train_loss.append(loss.item())\n","\n","      running_loss += (loss.item() * batch_size)\n","      dataset_size += batch_size\n","\n","      epoch_loss = running_loss / dataset_size\n","\n","      running_score.append(outputs.detach().cpu().numpy())\n","      running_score_y.append(targets.detach().cpu().numpy())\n","\n","      score = get_score(running_score_y, running_score)\n","\n","      bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n","                      Train_Acc=score[0],\n","                      Train_Auc=score[1],\n","                      LR=optimizer.param_groups[0]['lr']\n","                      )\n","    gc.collect()\n","    return epoch_loss, score\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1728483769879,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"-G6doI0alWI4"},"outputs":[],"source":["@torch.no_grad()\n","def valid_one_epoch(args, model, optimizer, valid_loader, epoch):\n","    model.eval()\n","\n","    dataset_size = 0\n","    running_loss = 0.0\n","    preds = []\n","    valid_targets = []\n","    softmax = nn.Softmax()\n","\n","    bar = tqdm(enumerate(valid_loader), total=len(valid_loader))\n","    for step, (images, targets) in enumerate(valid_loader):\n","      images = images.to(args[\"device\"])\n","      targets = targets.to(args[\"device\"])\n","      batch_size = targets.size(0)\n","      with torch.no_grad():\n","        outputs = model(images)\n","        predict = outputs.softmax(dim=1)\n","        loss = criterion(args, outputs, targets)\n","\n","      running_loss += (loss.item() * batch_size)\n","      dataset_size += batch_size\n","\n","      epoch_loss = running_loss / dataset_size\n","\n","      preds.append(predict.detach().cpu().numpy())\n","      valid_targets.append(targets.detach().cpu().numpy())\n","\n","      if len(set(np.concatenate(valid_targets))) == 1:\n","          continue\n","      score = get_score(valid_targets, preds)\n","\n","      bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n","                      Valid_Acc=score[0],\n","                      Valid_Auc=score[1],\n","                      LR=optimizer.param_groups[0]['lr'])\n","\n","    return epoch_loss, preds, valid_targets, score"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728483769879,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"0t9QvLWklbCr"},"outputs":[],"source":["def one_fold(model, optimizer, schedulerr, device, num_epochs, fold):\n","\n","    if torch.cuda.is_available():\n","        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n","\n","    start = time.time()\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_epoch_score = np.inf\n","    best_prediction = None\n","\n","    best_score = -np.inf\n","    for epoch in range(1, 1+num_epochs):\n","      train_epoch_loss, train_score = train_one_epoch(model, optimizer,\n","                                              train_loader=train_loader,\n","                                              device=device, epoch=epoch)\n","\n","      train_acc, train_auc = train_score\n","\n","      val_epoch_loss, predictions, valid_targets, valid_score = valid_one_epoch(ARGS,\n","                                                                                model,\n","                                                                                optimizer,\n","                                                                                valid_loader,\n","                                                                                epoch=epoch)\n","      valid_acc, valid_auc = valid_score\n","\n","      LOGGER.info(f'Epoch {epoch} - avg_train_loss: {train_epoch_loss:.4f}  avg_val_loss: {val_epoch_loss:.4f}')\n","      LOGGER.info(f'Epoch {epoch} - Train Acc: {train_acc:.4f}  Train Auc: {train_auc:.4f}  Valid Acc: {valid_acc:.4f}  Valid Auc: {valid_auc:.4f}')\n","\n","      if valid_auc >= best_score:\n","        best_score = valid_auc\n","\n","        print(f\"{b_}Validation Score Improved ({best_epoch_score} ---> {valid_auc})\")\n","        best_epoch_score = valid_auc\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","        # PATH = f\"Score-Fold-{fold}.bin\"\n","        PATH = ARGS[\"OUT_DIR\"] + f\"Score-Fold-{fold}.bin\"\n","        torch.save(model.state_dict(), PATH)\n","        # Save a model file from the current directory\n","        print(f\"Model Saved{sr_}\")\n","\n","        best_prediction = np.concatenate(predictions, axis=0)[:,1]\n","\n","    end = time.time()\n","    time_elapsed = end - start\n","\n","    LOGGER.info('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n","    LOGGER.info(\"Best Score: {:.4f}\".format(best_epoch_score))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, best_prediction, valid_targets"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728483769879,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"dSRnmiqbld8o"},"outputs":[],"source":["def create_model(args):\n","    model = models.vit_l_16(pretrained=True)\n","    model.heads[0] = torch.nn.Linear(in_features=model.heads[0].in_features, out_features=args[\"num_classes\"], bias=True)\n","    return model\n","\n","def create_model_timm(args):\n","    #model = timm.create_model(args[\"timm_model_name\"], pretrained=True, num_classes=args[\"num_classes\"])\n","    model = timm.create_model(args[\"timm_model_name\"], args[\"pretrained\"], num_classes=args[\"num_classes\"])\n","    # 重みを更新するパラメータを選択する\n","    # 最終層だけでOK\n","    \"\"\"\n","    params_to_update = []\n","    update_param_names = ['head.weight', 'head.bias']\n","\n","    for name, param in model.named_parameters():\n","        if name in update_param_names:\n","            param.requires_grad = True\n","            params_to_update.append(param)\n","        else:\n","            param.requires_grad = False\n","    \"\"\"\n","    return model\n","def criterion(args, outputs, labels, class_weights=None):\n","    if args['criterion'] == 'CrossEntropy':\n","      return nn.CrossEntropyLoss(weight=class_weights).to(args[\"device\"])(outputs, labels)\n","    elif args['criterion'] == \"None\":\n","        return None\n","\n","def fetch_optimizer(optimizer_parameters, lr, betas, optimizer_name=\"Adam\"):\n","    if optimizer_name == \"Adam\":\n","        optimizer = optim.Adam(optimizer_parameters, lr=lr)\n","    elif optimizer_name == \"AdamW\":\n","        optimizer = optim.AdamW(optimizer_parameters, lr=lr, betas=betas)\n","    return optimizer\n","\n","def fetch_scheduler(args, train_size, optimizer):\n","\n","    if args['scheduler'] == 'CosineAnnealingLR':\n","        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=args['T_max'],\n","                                                   eta_min=args['min_lr'])\n","    elif args['scheduler'] == 'CosineAnnealingWarmRestarts':\n","        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=args['T_0'],\n","                                                             eta_min=args['min_lr'])\n","    elif args['scheduler'] == \"None\":\n","        scheduler = None\n","\n","    return scheduler\n","\n","def get_score(y_trues, y_preds):\n","    predict_list, targets_list = np.concatenate(y_preds, axis=0), np.concatenate(y_trues)\n","    predict_list_proba = predict_list.copy()[:, 1]\n","    predict_list = predict_list.argmax(axis=1)\n","\n","    accuracy = accuracy_score(predict_list, targets_list)\n","    try:\n","        auc_score = roc_auc_score(targets_list, predict_list_proba)\n","    except:\n","        auc_score = 0.0\n","\n","    return (accuracy, auc_score)\n","\n","def prepare_loaders(args, train, image_transform, fold):\n","    df_train = train[train.kfold != fold].reset_index(drop=True)\n","    df_valid = train[train.kfold == fold].reset_index(drop=True)\n","\n","    train_dataset = CustomDataset(df_train, image_transform, data_type=\"train\")\n","    valid_dataset = CustomDataset(df_valid, image_transform, data_type=\"train\")\n","    #train_dataset = CustomDataset2(df_train, image_transform, data_type=\"train\", is_blurry=ARGS['is_blurry'])\n","    #valid_dataset = CustomDataset2(df_valid, image_transform, data_type=\"train\", is_blurry=ARGS['is_blurry'])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=args['train_batch_size'],\n","                              worker_init_fn=worker_init_fn(args[\"seed\"]),\n","                              num_workers=4,\n","                              shuffle=True, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset, batch_size=args['test_batch_size'],\n","                              num_workers=4,\n","                              shuffle=False, pin_memory=True)\n","\n","    return train_loader, valid_loader"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":179,"referenced_widgets":["d69f25abed104e339c23adc51e5ff7da","647b6ca15c444a3cb3fdaaecfb83dcdf","bf71a57cf16c431b8b81727c6b36ba75","34393600ba974eb08b8f9ea86cacea9b","2521b23523dd4e3eb778f40df0ea2cf9","6a05fb91b8a643d38b6305afe8d47268","e0cf3e0de68a413a82793eb109c97fc3","8c6811c7c6bf4fdfb7fda09439f0fcb8","b6dd16b27d6c4a1d991b761bbba94e82","f6599d195b4c4624b58dbd016998a735","f494163acdd34842b4a54a7b4a3be01d"]},"id":"O7l04skulfud","outputId":"4e9e2820-8514-4eed-fe6b-27a6f9598ff2"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33m====== Fold: 0 ======\u001b[0m\n","[INFO] Using GPU: Quadro P5000\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6485/6485 [44:52<00:00,  2.41it/s, Epoch=1, LR=1.02e-6, Train_Acc=0.973, Train_Auc=0.995, Train_Loss=0.0719]\n","  0%|          | 0/2162 [05:47<?, ?it/s, Epoch=1, LR=1.02e-6, Valid_Acc=0.99, Valid_Auc=1, Valid_Loss=0.0273]     \n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34mValidation Score Improved (inf ---> 0.9995855940165379)\n","Model Saved\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6485/6485 [44:53<00:00,  2.41it/s, Epoch=2, LR=9.92e-6, Train_Acc=0.991, Train_Auc=1, Train_Loss=0.0238]    \n","  0%|          | 0/2162 [05:45<?, ?it/s, Epoch=2, LR=9.92e-6, Valid_Acc=0.98, Valid_Auc=0.999, Valid_Loss=0.057]  \n"]},{"name":"stdout","output_type":"stream","text":["[7.9635785e-05 2.5443442e-05 3.8931146e-05 ... 9.9999964e-01 9.9970144e-01\n"," 9.9999905e-01]\n","\n","\u001b[33m====== Fold: 1 ======\u001b[0m\n","[INFO] Using GPU: Quadro P5000\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6485/6485 [44:59<00:00,  2.40it/s, Epoch=1, LR=1.02e-6, Train_Acc=0.974, Train_Auc=0.996, Train_Loss=0.0718]\n","  0%|          | 0/2162 [05:45<?, ?it/s, Epoch=1, LR=1.02e-6, Valid_Acc=0.991, Valid_Auc=1, Valid_Loss=0.0253]    \n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34mValidation Score Improved (inf ---> 0.9996083858967542)\n","Model Saved\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6485/6485 [44:56<00:00,  2.40it/s, Epoch=2, LR=9.92e-6, Train_Acc=0.993, Train_Auc=1, Train_Loss=0.0216]    \n","  0%|          | 0/2162 [05:45<?, ?it/s, Epoch=2, LR=9.92e-6, Valid_Acc=0.986, Valid_Auc=0.999, Valid_Loss=0.0373]\n"]},{"name":"stdout","output_type":"stream","text":["[3.7375910e-04 4.2870757e-05 2.9573182e-03 ... 9.9999762e-01 9.9994445e-01\n"," 9.9976391e-01]\n","\n"]}],"source":["train_copy = train.copy()\n","LOGGER.info(ARGS)\n","for fold in range(0, ARGS['n_fold']):\n","    print(f\"{y_}====== Fold: {fold} ======{sr_}\")\n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","\n","    # Create Dataloaders\n","    train_loader, valid_loader = prepare_loaders(args=ARGS, train=train, image_transform=image_transform, fold=fold)\n","    #train_loader, valid_loader = prepare_loaders(args=ARGS, train=train, image_transform=image_transform_train, fold=fold)\n","\n","    #vit_l16\n","    #model = create_model(ARGS)\n","    #heavy\n","    model = create_model_timm(ARGS)\n","    model = model.to(ARGS[\"device\"])\n","\n","    #損失関数・最適化関数の定義\n","    optimizer = fetch_optimizer(model.parameters(), optimizer_name=ARGS[\"optimizer\"], lr=ARGS[\"learning_rate\"], betas=(0.9, 0.999))\n","\n","    scheduler = fetch_scheduler(args=ARGS, train_size=len(train_loader), optimizer=optimizer)\n","\n","    model, predictions, targets = one_fold(model, optimizer, scheduler, device=ARGS[\"device\"], num_epochs=ARGS[\"epochs\"], fold=fold)\n","\n","    print(predictions)\n","    train_copy.loc[train_copy[train_copy.kfold == fold].index, \"oof\"] = predictions\n","    #train_copy.loc[train_copy[train_copy.kfold == fold].index, \"pred_0\"] = predictions[:,0]\n","    #train_copy.loc[train_copy[train_copy.kfold == fold].index, \"pred_1\"] = predictions[:,1]\n","\n","    del model, train_loader, valid_loader\n","    _ = gc.collect()\n","    torch.cuda.empty_cache()\n","    print()\n","\n","scores = roc_auc_score(train_copy[\"label\"].values, train_copy[\"oof\"].values)\n","LOGGER.info(f\"========== CV ==========\")\n","LOGGER.info(f\"CV: {scores:.4f}\")"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"CPr04XK_liYf"},"outputs":[],"source":["# OOF\n","#train_copy.to_csv(ARGS['OUT_DIR'] + f'oof.csv', index=False)\n","train_copy.to_csv(f\"{ARGS['OUT_DIR']}/oof_CV{scores:.4f}.csv\", index=False)\n","train_copy.to_csv(f\"{ARGS['DATA_DIR']}/oof_CV{scores:.4f}.csv\", index=False)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"a-K5oJjtlj6l"},"outputs":[],"source":["import os\n","os.environ['oof_CVSroce'] = f\"{scores:.4f}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ig1QcrNVlmUR"},"outputs":[],"source":["!cp -rf ./oof_CV${oof_CVSroce}.csv /content/drive/MyDrive/SIGNATE_TECNO/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VeqrJr1glo53"},"outputs":[],"source":["!ls /content/drive/MyDrive/SIGNATE_TENCO"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"vQ1MpTcxlqbG"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>file</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>222i9jkyZCwNfVhq9RkAEz.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2264fFtWKWz5P4SzM9zFH6.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>226aR4s3hWSLpmjbWVUvpT.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>226EdDxcWUiPJynRuWKbfn.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   image_path\n","0                        file\n","1  222i9jkyZCwNfVhq9RkAEz.jpg\n","2  2264fFtWKWz5P4SzM9zFH6.jpg\n","3  226aR4s3hWSLpmjbWVUvpT.jpg\n","4  226EdDxcWUiPJynRuWKbfn.jpg"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["#sample_submit.csvを読み込みます\n","#import pandas as pd\n","#submit = pd.read_csv(f\"{ARGS['DATA_DIR']}/sample_submit.csv\", header=None)\n","#submit = pd.read_csv(f\"/content/drive/MyDrive/SIGNATE/Sense/sample_submit.csv\", header=['image_path', 'label'])\n","#submit = pd.read_csv(f\"/content/drive/MyDrive/SIGNATE/Sense/sample_submit.csv\", header=None, names=['image_path', 'label'])\n","test = pd.read_csv(f\"/root/signate_tecno/input/test.csv\", header=None, names=['image_path'])\n","submit = pd.read_csv(f\"/root/signate_tecno/input/sample_submit.csv\", header=None, names=['image_path', 'label'])\n","\n","test.head()\n","#submit.head()"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"NbdzRLMMlthV"},"outputs":[],"source":["# test用のデータ拡張\n","image_transform_test = albu.Compose([\n","    albu.Resize(ARGS[\"image_size\"][0], ARGS[\"image_size\"][1]),\n","    albu.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    AT.ToTensorV2 ()\n","    ])\n","test_dataset = CustomDataset(submit, image_transform_test, data_type=\"test\")\n","#test_dataset = CustomDataset2(submit, image_transform_test, data_type=\"test\", is_blurry=ARGS['is_blurry'])\n","test_loader = DataLoader(test_dataset, batch_size=ARGS[\"test_batch_size\"], shuffle=False, num_workers=1) # 4\n","#"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"FEX-PZJslvsI"},"outputs":[],"source":["@torch.no_grad()\n","def valid_fn(model, dataloader, device):\n","    model.eval()\n","\n","    dataset_size = 0\n","    running_loss = 0.0\n","\n","    predict_list = []\n","\n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for step, images in bar:\n","        images = images.to(device)\n","        with torch.no_grad():\n","            outputs = model(images)\n","            #出力にソフトマックス関数を適用\n","            predicts = outputs.softmax(dim=1)\n","\n","        predicts = predicts.cpu().detach().numpy()\n","        predict_list.append(predicts)\n","    predict_list = np.concatenate(predict_list, axis=0)\n","    #予測値が1である確率を提出します。\n","    predict_list = predict_list[:, 1]\n","    gc.collect()\n","\n","    return predict_list"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"bs21fik7lxvM"},"outputs":[],"source":["def inference(model_paths, dataloader, device):\n","    final_preds = []\n","    ARGS['pretrained'] = False\n","    for i, path in enumerate(model_paths):\n","        #model = create_model(ARGS)\n","        model = create_model_timm(ARGS)\n","        model = model.to(device)\n","\n","        #学習済みモデルの読み込み\n","        model.load_state_dict(torch.load(path))\n","        model.eval()\n","\n","        print(f\"Getting predictions for model {i+1}\")\n","        preds = valid_fn(model, dataloader, device)\n","        final_preds.append(preds)\n","\n","    final_preds = np.array(final_preds)\n","    final_preds = np.mean(final_preds, axis=0)\n","    return final_preds"]},{"cell_type":"code","execution_count":83,"metadata":{"id":"y6ywuPBPlzlQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["/root/signate_tecno/output\n","'base copy.ipynb'\t  exp003_Learning_CV.ipynb\n"," base.ipynb\t\t  exp004_Learning_segment.ipynb\n"," base.py\t\t  exp005_detection.ipynb\n","'exp001 Learning.ipynb'   exp006_signate_baseline.ipynb\n"," exp001_Inference.ipynb   sam2_b.pt\n"," exp002_Learning.ipynb\t  yolov8n.pt\n"]}],"source":["#!ls /content/test\n","print(f\"{ARGS['OUT_DIR']}\")\n","!ls ./\n","#!cp -rf ./Score-Fold-0.bin /content/drive/MyDrive/SIGNATE/Sense\n","#!cp -rf ./Score-Fold-1.bin /content/drive/MyDrive/SIGNATE/Sense\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"q3cRS1y2l04v"},"outputs":[],"source":["MODEL_PATHS = [\n","    f\"/root/signate_tecno/outputScore-Fold-{i}.bin\" for i in range(ARGS[\"n_fold\"])\n","]"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"-bnJT9a4l26H"},"outputs":[{"name":"stdout","output_type":"stream","text":["Getting predictions for model 1\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2287/2287 [05:57<00:00,  6.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Getting predictions for model 2\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2287/2287 [05:57<00:00,  6.40it/s]\n"]}],"source":["predict_list = inference(MODEL_PATHS, test_loader, ARGS[\"device\"])"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Count of values in predict_list between 0.4 and 0.6: 427\n"]}],"source":["count = np.sum((predict_list >= 0.4) & (predict_list < 0.6))\n","print(f\"Count of values in predict_list between 0.4 and 0.6: {count}\")"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"YkKvA0Nxl4Lj"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_path</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>222i9jkyZCwNfVhq9RkAEz.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2264fFtWKWz5P4SzM9zFH6.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>226aR4s3hWSLpmjbWVUvpT.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>226EdDxcWUiPJynRuWKbfn.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>22DuBz5yNJvDzcrxiw3Kqy.jpg</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   image_path  label\n","0  222i9jkyZCwNfVhq9RkAEz.jpg      1\n","1  2264fFtWKWz5P4SzM9zFH6.jpg      0\n","2  226aR4s3hWSLpmjbWVUvpT.jpg      1\n","3  226EdDxcWUiPJynRuWKbfn.jpg      0\n","4  22DuBz5yNJvDzcrxiw3Kqy.jpg      0"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["#submit['label'] = predict_list\n","submit['label'] = (predict_list > 0.5)\n","submit['label'] = submit['label'].astype(int)\n","submit.head()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["submit.to_csv(f'{ARGS[\"DATA_DIR\"]}/submission_CV_v3.csv',index = False, header = None)\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["submit[\"predict_proba\"] = predict_list"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'submit' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msubmit\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'submit' is not defined"]}],"source":["submit"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 15.88 GiB of which 16.50 MiB is free. Process 21524 has 7.51 GiB memory in use. Process 28218 has 4.57 GiB memory in use. Process 6022 has 3.79 GiB memory in use. Of the allocated memory 7.01 GiB is allocated by PyTorch, and 369.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[98], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BlipProcessor, BlipForConditionalGeneration\n\u001b[1;32m      5\u001b[0m processor \u001b[38;5;241m=\u001b[39m BlipProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/blip-image-captioning-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBlipForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalesforce/blip-image-captioning-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m submit\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.6\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.4\u001b[39m:\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/transformers/modeling_utils.py:2958\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2954\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2955\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2956\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2957\u001b[0m         )\n\u001b[0;32m-> 2958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","    \u001b[0;31m[... skipping similar frames: Module._apply at line 780 (3 times)]\u001b[0m\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 15.88 GiB of which 16.50 MiB is free. Process 21524 has 7.51 GiB memory in use. Process 28218 has 4.57 GiB memory in use. Process 6022 has 3.79 GiB memory in use. Of the allocated memory 7.01 GiB is allocated by PyTorch, and 369.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["import requests\n","from PIL import Image\n","from transformers import BlipProcessor, BlipForConditionalGeneration\n","\n","processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n","model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\")\n","for i, row in submit.iterrows():\n","    if row[\"predict_proba\"] < 0.6 and row[\"predict_proba\"] > 0.4:\n","        img_url = f'/root/signate_tecno/input/test/{row[\"image_path\"]}'\n","        print(img_url) \n","        raw_image = Image.open(img_url).convert('RGB')\n","        # conditional image captioning\n","        text = \"a photography of a human with his hands\"\n","        inputs = processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\")\n","        out = model.generate(**inputs)\n","        print(processor.decode(out[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNpSwWm7kv4yVjyhBkgRaJk","gpuType":"A100","machine_shape":"hm","mount_file_id":"140HEFTP_5eKP-RsCeaydYc-clUD8Qp6n","provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2521b23523dd4e3eb778f40df0ea2cf9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34393600ba974eb08b8f9ea86cacea9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6599d195b4c4624b58dbd016998a735","placeholder":"​","style":"IPY_MODEL_f494163acdd34842b4a54a7b4a3be01d","value":" 1.22G/1.22G [02:05&lt;00:00, 12.2MB/s]"}},"647b6ca15c444a3cb3fdaaecfb83dcdf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a05fb91b8a643d38b6305afe8d47268","placeholder":"​","style":"IPY_MODEL_e0cf3e0de68a413a82793eb109c97fc3","value":"model.safetensors: 100%"}},"6a05fb91b8a643d38b6305afe8d47268":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c6811c7c6bf4fdfb7fda09439f0fcb8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6dd16b27d6c4a1d991b761bbba94e82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bf71a57cf16c431b8b81727c6b36ba75":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c6811c7c6bf4fdfb7fda09439f0fcb8","max":1220365908,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6dd16b27d6c4a1d991b761bbba94e82","value":1220365908}},"d69f25abed104e339c23adc51e5ff7da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_647b6ca15c444a3cb3fdaaecfb83dcdf","IPY_MODEL_bf71a57cf16c431b8b81727c6b36ba75","IPY_MODEL_34393600ba974eb08b8f9ea86cacea9b"],"layout":"IPY_MODEL_2521b23523dd4e3eb778f40df0ea2cf9"}},"e0cf3e0de68a413a82793eb109c97fc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f494163acdd34842b4a54a7b4a3be01d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6599d195b4c4624b58dbd016998a735":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
