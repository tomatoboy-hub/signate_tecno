{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 変更点\n","[ver1.0] Albumentationをがっつり行う\n","    LSTMを層に加えるのをやってみる\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3325,"status":"ok","timestamp":1728483756778,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"LVh0Z4vGk-uu","outputId":"851eb8b9-81c6-4b1c-fd82-a9210b1313fe"},"outputs":[],"source":["import os\n","import shutil\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","classifications = [\"not-hold a folding fan\", \"hold a folding fan\"]\n","\n","def search_images(directory):\n","    # 対応する画像の拡張子を定義\n","    image_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n","\n","    # 画像ファイルのパスとファイル名をリストに格納\n","    image_files = []\n","\n","    for root, dirs, files in os.walk(directory):\n","        for file in files:\n","            if file.lower().endswith(image_extensions):\n","                file_path = os.path.join(root, file)\n","                image_files.append({\n","                    'file_name': file,\n","                    'file_path': file_path\n","                })\n","\n","    # pandas DataFrame に変換\n","    df = pd.DataFrame(image_files)\n","\n","    return df\n","\n","#HOME = Path(\"/content\")\n","#INPUTS = HOME / \"dataset\"  # input data\n","INPUTS = Path(\"/root/signate_tecno/input\")\n","TRAIN_IMAGEDIR0 = INPUTS / \"train\" / \"not-hold\"\n","TRAIN_IMAGEDIR1 = INPUTS / \"train\" / \"hold\"\n","\n","train_df = pd.DataFrame()\n","train_df0 = search_images(TRAIN_IMAGEDIR0)\n","train_df1 = search_images(TRAIN_IMAGEDIR1)\n","train_df0['caption'] = classifications[0]\n","train_df1['caption'] = classifications[1]\n","train_df0['label'] = 0\n","train_df1['label'] = 1\n","train_df = pd.concat([train_df0, train_df1], axis=0)\n","print(train_df)\n","train_df.to_csv('/root/signate_tecno/input/train.csv', index=None)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7698,"status":"ok","timestamp":1728483767832,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"Dz7q0RKxlCdY"},"outputs":[],"source":["import os, shutil\n","\n","#必要なライブラリのインポート\n","import re, gc, sys\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from glob import glob\n","\n","import warnings, random\n","import cv2\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","from sklearn.model_selection import StratifiedKFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim import lr_scheduler\n","\n","import torchvision\n","from torchvision import transforms\n","import torchvision.models as models\n","from torch.cuda.amp import GradScaler\n","\n","import timm\n","import yaml\n","from tqdm import tqdm\n","import time\n","import copy\n","from collections import defaultdict\n","\n","from colorama import Fore, Back, Style\n","b_ = Fore.BLUE\n","y_ = Fore.YELLOW\n","sr_ = Style.RESET_ALL"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1728483767833,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"uzZjaObUlE0F","outputId":"b25284d8-0731-4c22-98d8-eb4dce27b2d0"},"outputs":[],"source":["ARGS = {\n","  'DATA_DIR': '/root/signate_tecno/input/',\n","  'OUT_DIR': '/root/signate_tecno/output',\n","  'model_name': 'vit_l_16',\n","  'image_size': (224, 224), # vit_l16\n","  #cpu(slow)\n","  #'train_batch_size': 4,\n","  #'test_batch_size': 8,\n","  #gpu\n","  #'train_batch_size': 28, # 32(x)\n","  #'test_batch_size': 56,\n","  #'n_fold': 2,\n","  #'epochs': 3,\n","  #'timm_model_name': 'resnet50',\n","  #'timm_model_name': 'vit_base_patch16_224',\n","  #lb:0.8545424 ? :\n","  #'timm_model_name': 'vit_large_patch32_224_in21k',\n","  #down?\n","  #'timm_model_name': 'vit_huge_patch14_224_in21k',\n","  #colab free crash memory?.(batch=4)\n","  #'timm_model_name': 'vit_giant_patch14_224_clip_laion2b',\n","  #Only one class present in y_true. ROC AUC score is not defined in that case.\n","  #lb:0.8944379  : top, batchsize : 4, 'image_size': (448, 448),\n","  #'timm_model_name': 'eva02_large_patch14_448.mim_m38m_ft_in22k_in1k',\n","  'timm_model_name': 'swin_small_patch4_window7_224.ms_in22k_ft_in1k',\n","  'pretrained': True,\n","  'n_fold': 2, # 5\n","  'epochs': 2, # 8\n","  #'image_size': (448, 448), # eva02_large_patch14_448\n","  'criterion': 'CrossEntropy',\n","  #'is_blurry': True,\n","  'is_blurry': False,\n","  #'image_size': (336, 336),\n","  #GPU: 16GB\n","  'train_batch_size': 4, # 4, #1(ng?)\n","  'test_batch_size': 15, #x3?\n","  #GPU: 80GB?\n","  #'train_batch_size': 32, # 4, #1(ng?)\n","  #'test_batch_size': 96,\n","  #batchsize row is roc_auc calc ng,(batchsizeが小さいと、class data が片方のみ存在している状態になり roc_auc の計算ができない)\n","  'seed': 2023,\n","  'optimizer': 'AdamW',\n","  'learning_rate': 1e-05,\n","  'scheduler': 'CosineAnnealingLR', # CosineAnnealingWarmRestarts\n","  'min_lr': 1e-06,\n","  'T_max': 500,\n","  'n_accumulate': 1,\n","  'clip_grad_norm': 'None',\n","  'apex': True,\n","  'num_classes': 2,\n","  'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n","  'aug_prob': 0.75,\n","  'cutmix_prob':0.5\n","  }\n","ARGS"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1728483767833,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"aWqK502-lHv0"},"outputs":[],"source":["def get_logger(filename):\n","    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler2 = FileHandler(filename=f\"{filename}.log\")\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler2)\n","    return logger\n","\n","#再現性を出すために必要な関数となります\n","def worker_init_fn(worker_id):\n","    torch.manual_seed(worker_id)\n","    random.seed(worker_id)\n","    np.random.seed(worker_id)\n","    torch.cuda.manual_seed(worker_id)\n","    os.environ['PYTHONHASHSEED'] = str(worker_id)\n","\n","def set_seed(seed=42):\n","    '''Sets the seed of the entire notebook so results are the same every time we run.\n","    This is for REPRODUCIBILITY.'''\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    # When running on the CuDNN backend, two further options must be set\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Set a fixed value for the hash seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","\n","LOGGER = get_logger(ARGS['OUT_DIR']+'train')\n","set_seed(ARGS[\"seed\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1728483767833,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"qHafV2PtlI_x","outputId":"99180516-c86b-45b9-ac66-0d9a4cc1e8ad"},"outputs":[],"source":["def create_folds(data, num_splits, seed):\n","    data[\"kfold\"] = -1\n","\n","    mskf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=seed)\n","    labels = [\"label\"]\n","    data_labels = data[labels].values\n","\n","    for f, (t_, v_) in enumerate(mskf.split(data, data_labels)):\n","        data.loc[v_, \"kfold\"] = f\n","\n","    return data\n","\n","train = pd.read_csv(f\"{ARGS['DATA_DIR']}/train.csv\")\n","train = create_folds(train, num_splits=ARGS[\"n_fold\"], seed=ARGS[\"seed\"])\n","print(\"Folds created successfully\")\n","\n","train.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1728483767833,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"dedN5t8KlNLX"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, df, transform, data_type):\n","        self.df = df\n","        self.data_type = data_type\n","\n","        if self.data_type == \"train\":\n","            self.image_paths = df['file_path']\n","            self.labels = df['label']\n","        if self.data_type == \"test\":\n","            #self.image_paths = df[0]\n","            self.image_paths = df['image_path']\n","\n","        self.transform= transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index: int):\n","        image_path = self.image_paths[index]\n","\n","        if self.data_type == \"train\":\n","            #image = cv2.imread(f\"/content/train/{file_name}\")\n","            image = cv2.imread(f\"{image_path}\")\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            label = self.labels[index]\n","            label = torch.tensor(label, dtype=torch.long)\n","\n","            image = self.transform(image=image)[\"image\"]\n","            return image, label\n","\n","        if self.data_type == \"test\":\n","            if os.path.exists(f\"/root/signate_tecno/input/test/{image_path}\"):\n","              image = cv2.imread(f\"/root/signate_tecno/input/test/{image_path}\")\n","              image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","              image = self.transform(image=image)[\"image\"]\n","            else:\n","              print(\"not found \" + image_path)\n","              image = None\n","\n","            return image"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728483768948,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"S2oGvt1jlO97"},"outputs":[],"source":["class CustomDataset2(Dataset):\n","    def __init__(self, df, transform, data_type, is_blurry):\n","        self.df = df\n","        self.data_type = data_type\n","\n","        if self.data_type == \"train\":\n","            if is_blurry:\n","                self.image_paths = df['file_path']\n","            else:\n","                self.image_paths = df['file_path']\n","            self.labels = df['label']\n","        if self.data_type == \"test\":\n","            #self.image_paths = df[0]\n","            self.image_paths = df['image_path']\n","\n","        self.transform= transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index: int):\n","        image_path = self.image_paths[index]\n","\n","        if self.data_type == \"train\":\n","            image = cv2.imread(f\"/content/train/{image_path}\")\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            label = self.labels[index]\n","            label = torch.tensor(label, dtype=torch.long)\n","\n","            image = self.transform(image=image)[\"image\"]\n","            return image, label\n","\n","        if self.data_type == \"test\":\n","            image = cv2.imread(f\"/content/test/{image_path}\")\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            image = self.transform(image=image)[\"image\"]\n","\n","            return image"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":932,"status":"ok","timestamp":1728483769879,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"e7QY4F7alQeZ"},"outputs":[],"source":["import albumentations as A\n","from albumentations.pytorch import transforms as AT\n","\n","# Augumentation用\n","#CV : up, LB : down\n","image_transform_train = A.Compose([\n","            A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=ARGS['aug_prob']),\n","            A.OneOf([\n","                A.MotionBlur(blur_limit=5),\n","                A.MedianBlur(blur_limit=5),\n","                A.GaussianBlur(blur_limit=5),\n","                A.GaussNoise(var_limit=(5.0, 30.0)),\n","            ], p=ARGS['aug_prob']),\n","\n","            A.OneOf([\n","                A.OpticalDistortion(distort_limit=1.0),\n","                A.GridDistortion(num_steps=5, distort_limit=1.),\n","                A.ElasticTransform(alpha=3),\n","            ], p=ARGS['aug_prob']),\n","\n","            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=ARGS['aug_prob']),\n","            A.Resize(ARGS[\"image_size\"][0], ARGS[\"image_size\"][1]),\n","            A.CoarseDropout(max_holes=16, max_height=64, max_width=64, min_holes=1, min_height=8, min_width=8, p=ARGS['aug_prob']),    \n","            A.Normalize(mean=0.5, std=0.5),\n","            AT.ToTensorV2()\n","        ])\n","\"\"\"\n","image_transform_train = A.Compose([\n","    A.Resize(ARGS[\"image_size\"][0], ARGS[\"image_size\"][1]),\n","    A.HorizontalFlip(p=0.5),\n","    A.VerticalFlip(p=0.5),\n","    A.RandomBrightnessContrast(p=0.3),\n","    A.RandomGamma(gamma_limit=(85, 115), p=0.3),\n","    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.10, rotate_limit=45, p=0.5),\n","    A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    AT.ToTensorV2()\n","])\n","\"\"\"\n","image_transform = A.Compose([\n","    A.Resize(ARGS[\"image_size\"][0], ARGS[\"image_size\"][1]),\n","    # albu.HorizontalFlip(p=0.5),\n","    # albu.VerticalFlip(p=0.5),\n","    # albu.RandomBrightnessContrast(p=0.3),\n","    # albu.RandomGamma(gamma_limit=(85, 115), p=0.3),\n","    # albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.10, rotate_limit=45, p=0.5),\n","    A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    AT.ToTensorV2()\n","])"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","\n","def mixup_data(x, y, alpha=1.0):\n","    '''Returns mixed inputs, pairs of targets, and lambda'''\n","    if alpha > 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1\n","\n","    batch_size = x.size()[0]\n","    index = torch.randperm(batch_size).to(x.device)\n","\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam\n","\n","def mixup_criterion(criterion, pred, y_a, y_b, lam):\n","    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def rand_bbox(size, lam):\n","    W = size[2]\n","    H = size[3]\n","    cut_rat = np.sqrt(1. - lam)\n","    cut_w = np.int32(W * cut_rat)\n","    cut_h = np.int32(H * cut_rat)\n","\n","    # ランダムな切り取り位置の計算\n","    cx = np.random.randint(W)\n","    cy = np.random.randint(H)\n","\n","    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n","    bby1 = np.clip(cy - cut_h // 2, 0, H)\n","    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n","    bby2 = np.clip(cy + cut_h // 2, 0, H)\n","\n","    return bbx1, bby1, bbx2, bby2\n","\n","def cutmix_data(x, y, alpha=1.0, device='cuda'):\n","    '''CutMixのデータ生成'''\n","    if alpha > 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1.0\n","\n","    batch_size = x.size()[0]\n","    index = torch.randperm(batch_size).to(device)\n","\n","    y_a, y_b = y, y[index]\n","\n","    # ランダムな矩形領域の取得\n","    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n","    # 画像の切り貼り\n","    x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]\n","    # λを修正\n","    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n","    return x, y_a, y_b, lam\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1728483769879,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"-r0OlLCilSTC"},"outputs":[],"source":["def train_one_epoch(model, optimizer, train_loader, device, epoch):\n","    model.train()\n","    dataset_size = 0\n","    running_loss = 0.0\n","    running_score = []\n","    running_score_y = []\n","    scaler = GradScaler(enabled=ARGS[\"apex\"])\n","\n","    train_loss = []\n","    bar = tqdm(enumerate(train_loader), total=len(train_loader))\n","    for step, (images, targets) in bar:\n","      images = images.to(device)\n","      targets = targets.to(device)\n","\n","      batch_size = targets.size(0)\n","      num_classes = 2\n","      targets = F.one_hot(targets, num_classes=num_classes).float()\n","\n","      r = np.random.rand()\n","      if r < ARGS[\"cutmix_prob\"]:\n","        images, targets_a, targets_b, lam = cutmix_data(images, targets, alpha=1.0, device=device)\n","        with torch.cuda.amp.autocast(enabled=ARGS[\"apex\"]):\n","            outputs = model(images)        \n","            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n","      else:\n","        with torch.cuda.amp.autocast(enabled=ARGS[\"apex\"]):\n","            outputs = model(images)\n","            loss = criterion(outputs, targets)\n","\n","      scaler.scale(loss).backward()\n","\n","      if ARGS[\"clip_grad_norm\"] != \"None\":\n","          grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), ARGS[\"clip_grad_norm\"])\n","      else:\n","          grad_norm = None\n","\n","      scaler.step(optimizer)\n","      scaler.update()\n","\n","      optimizer.zero_grad()\n","\n","      if scheduler is not None:\n","          scheduler.step()\n","\n","      train_loss.append(loss.item())\n","\n","      running_loss += (loss.item() * batch_size)\n","      dataset_size += batch_size\n","\n","      epoch_loss = running_loss / dataset_size\n","\n","      running_score.append(outputs.detach().cpu().numpy())\n","      running_score_y.append(targets.detach().cpu().numpy())\n","\n","      score = get_score(running_score_y, running_score)\n","\n","      bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n","                      Train_Acc=score[0],\n","                      Train_Auc=score[1],\n","                      LR=optimizer.param_groups[0]['lr']\n","                      )\n","    gc.collect()\n","    return epoch_loss, score\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1728483769879,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"-G6doI0alWI4"},"outputs":[],"source":["@torch.no_grad()\n","def valid_one_epoch(args, model, optimizer, valid_loader, epoch):\n","    model.eval()\n","\n","    dataset_size = 0\n","    running_loss = 0.0\n","    preds = []\n","    valid_targets = []\n","    softmax = nn.Softmax()\n","\n","    bar = tqdm(enumerate(valid_loader), total=len(valid_loader))\n","    for step, (images, targets) in enumerate(valid_loader):\n","      images = images.to(args[\"device\"])\n","      targets = targets.to(args[\"device\"])\n","      batch_size = targets.size(0)\n","      num_classes = 2\n","      targets = F.one_hot(targets, num_classes=num_classes).float()\n","      with torch.no_grad():\n","        outputs = model(images)\n","        \n","        predict = outputs.softmax(dim=1)\n","        loss = criterion( outputs, targets)\n","\n","      running_loss += (loss.item() * batch_size)\n","      dataset_size += batch_size\n","\n","      epoch_loss = running_loss / dataset_size\n","\n","      preds.append(predict.detach().cpu().numpy())\n","      valid_targets.append(targets.detach().cpu().numpy())\n","      # ターゲットをクラスラベルに変換\n","      targets_array = np.concatenate(valid_targets)\n","      targets_labels = np.argmax(targets_array, axis=1)\n","\n","        # ターゲットのユニークなクラス数を確認\n","      if len(np.unique(targets_labels)) == 1:\n","        continue\n","      score = get_score(valid_targets, preds)\n","\n","      bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n","                      Valid_Acc=score[0],\n","                      Valid_Auc=score[1],\n","                      LR=optimizer.param_groups[0]['lr'])\n","\n","    return epoch_loss, preds, valid_targets, score"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728483769879,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"0t9QvLWklbCr"},"outputs":[],"source":["def one_fold(model, optimizer, schedulerr, device, num_epochs, fold):\n","\n","    if torch.cuda.is_available():\n","        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n","\n","    start = time.time()\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_epoch_score = np.inf\n","    best_prediction = None\n","\n","    best_score = -np.inf\n","    for epoch in range(1, 1+num_epochs):\n","      train_epoch_loss, train_score = train_one_epoch(model, optimizer,\n","                                              train_loader=train_loader,\n","                                              device=device, epoch=epoch)\n","\n","      train_acc, train_auc = train_score\n","\n","      val_epoch_loss, predictions, valid_targets, valid_score = valid_one_epoch(ARGS,\n","                                                                                model,\n","                                                                                optimizer,\n","                                                                                valid_loader,\n","                                                                                epoch=epoch)\n","      valid_acc, valid_auc = valid_score\n","\n","      LOGGER.info(f'Epoch {epoch} - avg_train_loss: {train_epoch_loss:.4f}  avg_val_loss: {val_epoch_loss:.4f}')\n","      LOGGER.info(f'Epoch {epoch} - Train Acc: {train_acc:.4f}  Train Auc: {train_auc:.4f}  Valid Acc: {valid_acc:.4f}  Valid Auc: {valid_auc:.4f}')\n","\n","      if valid_auc >= best_score:\n","        best_score = valid_auc\n","\n","        print(f\"{b_}Validation Score Improved ({best_epoch_score} ---> {valid_auc})\")\n","        best_epoch_score = valid_auc\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","        # PATH = f\"Score-Fold-{fold}.bin\"\n","        PATH = ARGS[\"OUT_DIR\"] + f\"/Score-Fold-{fold}-effnet_lstm.bin\"\n","        torch.save(model.state_dict(), PATH)\n","        # Save a model file from the current directory\n","        print(f\"Model Saved{sr_}\")\n","\n","        best_prediction = np.concatenate(predictions, axis=0)[:,1]\n","\n","    end = time.time()\n","    time_elapsed = end - start\n","\n","    LOGGER.info('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n","    LOGGER.info(\"Best Score: {:.4f}\".format(best_epoch_score))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, best_prediction, valid_targets"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["class CustomModelWithLSTM(nn.Module):\n","    def __init__(self, args):\n","        super(CustomModelWithLSTM, self).__init__()\n","        # Load timm model\n","        self.model = timm.create_model(args[\"timm_model_name\"], pretrained=args[\"pretrained\"], num_classes=0)  # num_classes=0 to output raw features\n","\n","        # Add LSTM layer\n","        self.lstm = nn.LSTM(input_size=self.model.num_features, hidden_size=224, num_layers=2, batch_first=True)\n","\n","        # Final classification layer (binary classification)\n","        self.classifier = nn.Linear(224 , args[\"num_classes\"])\n","\n","    def forward(self, x):\n","        # Pass through timm model\n","        x = self.model(x)  # shape: (batch_size, num_features)\n","        \n","        # Add LSTM layer\n","        # Reshaping to (batch_size, sequence_length=1, num_features) to fit LSTM input requirements\n","        x = x.unsqueeze(1)\n","        lstm_out, (hn, cn) = self.lstm(x)\n","\n","        # Classifier layer (using the last hidden state from LSTM)\n","        out = self.classifier(hn[-1])\n","        return out\n","\n","def create_model_timm(args):\n","    return CustomModelWithLSTM(args)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728483769879,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"dSRnmiqbld8o"},"outputs":[],"source":["def create_model(args):\n","    model = models.vit_l_16(pretrained=True)\n","    model.heads[0] = torch.nn.Linear(in_features=model.heads[0].in_features, out_features=args[\"num_classes\"], bias=True)\n","    return model\n","\"\"\"\n","def create_model_timm(args):\n","    #model = timm.create_model(args[\"timm_model_name\"], pretrained=True, num_classes=args[\"num_classes\"])\n","    model = timm.create_model(args[\"timm_model_name\"], args[\"pretrained\"], num_classes=args[\"num_classes\"])\n","    # 重みを更新するパラメータを選択する\n","    # 最終層だけでOK\n","    params_to_update = []\n","    update_param_names = ['head.weight', 'head.bias']\n","\n","    for name, param in model.named_parameters():\n","        if name in update_param_names:\n","            param.requires_grad = True\n","            params_to_update.append(param)\n","        else:\n","            param.requires_grad = False\n","    \n","    return model\n","\"\"\"\n","\"\"\"\n","def criterion(args, outputs, labels, class_weights=None):\n","    if args['criterion'] == 'CrossEntropy':\n","      return nn.CrossEntropyLoss(weight=class_weights).to(args[\"device\"])(outputs, labels)\n","    elif args['criterion'] == \"None\":\n","        return None\n","\"\"\"\n","\n","criterion = nn.BCEWithLogitsLoss().to(ARGS[\"device\"])\n","\n","\n","def fetch_optimizer(optimizer_parameters, lr, betas, optimizer_name=\"Adam\"):\n","    if optimizer_name == \"Adam\":\n","        optimizer = optim.Adam(optimizer_parameters, lr=lr)\n","    elif optimizer_name == \"AdamW\":\n","        optimizer = optim.AdamW(optimizer_parameters, lr=lr, betas=betas)\n","    return optimizer\n","\n","def fetch_scheduler(args, train_size, optimizer):\n","\n","    if args['scheduler'] == 'CosineAnnealingLR':\n","        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=args['T_max'],\n","                                                   eta_min=args['min_lr'])\n","    elif args['scheduler'] == 'CosineAnnealingWarmRestarts':\n","        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=args['T_0'],\n","                                                             eta_min=args['min_lr'])\n","    elif args['scheduler'] == \"None\":\n","        scheduler = None\n","\n","    return scheduler\n","\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","\n","def get_score(targets_list, predict_list):\n","    # 予測値とターゲットをnumpy配列に変換\n","    targets_array = np.concatenate(targets_list, axis=0)\n","    preds_array = np.concatenate(predict_list, axis=0)\n","\n","    # ターゲットと予測値をクラスラベルに変換\n","    # ターゲット（one-hotエンコーディングされたラベル）からクラスラベルを取得\n","    targets_labels = np.argmax(targets_array, axis=1)\n","\n","    # 予測確率からクラスラベルを取得\n","    preds_labels = np.argmax(preds_array, axis=1)\n","\n","    # 予測確率（正例の確率）を取得（必要に応じて）\n","    preds_proba = preds_array[:, 1]\n","\n","    # 精度の計算\n","    accuracy = accuracy_score(targets_labels, preds_labels)\n","\n","    # AUCの計算（2クラス分類の場合）\n","    try:\n","        auc_score = roc_auc_score(targets_labels, preds_proba)\n","    except ValueError:\n","        auc_score = np.nan  # AUCが計算できない場合\n","\n","    return accuracy, auc_score\n","\n","\n","def prepare_loaders(args, train, image_transform, fold):\n","    df_train = train[train.kfold != fold].reset_index(drop=True)\n","    df_valid = train[train.kfold == fold].reset_index(drop=True)\n","\n","    train_dataset = CustomDataset(df_train, image_transform, data_type=\"train\")\n","    valid_dataset = CustomDataset(df_valid, image_transform, data_type=\"train\")\n","    #train_dataset = CustomDataset2(df_train, image_transform, data_type=\"train\", is_blurry=ARGS['is_blurry'])\n","    #valid_dataset = CustomDataset2(df_valid, image_transform, data_type=\"train\", is_blurry=ARGS['is_blurry'])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=args['train_batch_size'],\n","                              worker_init_fn=worker_init_fn(args[\"seed\"]),\n","                              num_workers=4,\n","                              shuffle=True, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset, batch_size=args['test_batch_size'],\n","                              num_workers=4,\n","                              shuffle=False, pin_memory=True)\n","\n","    return train_loader, valid_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":179,"referenced_widgets":["d69f25abed104e339c23adc51e5ff7da","647b6ca15c444a3cb3fdaaecfb83dcdf","bf71a57cf16c431b8b81727c6b36ba75","34393600ba974eb08b8f9ea86cacea9b","2521b23523dd4e3eb778f40df0ea2cf9","6a05fb91b8a643d38b6305afe8d47268","e0cf3e0de68a413a82793eb109c97fc3","8c6811c7c6bf4fdfb7fda09439f0fcb8","b6dd16b27d6c4a1d991b761bbba94e82","f6599d195b4c4624b58dbd016998a735","f494163acdd34842b4a54a7b4a3be01d"]},"id":"O7l04skulfud","outputId":"4e9e2820-8514-4eed-fe6b-27a6f9598ff2"},"outputs":[],"source":["train_copy = train.copy()\n","LOGGER.info(ARGS)\n","for fold in range(0, ARGS['n_fold']):\n","    print(f\"{y_}====== Fold: {fold} ======{sr_}\")\n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","\n","    # Create Dataloaders\n","    train_loader, valid_loader = prepare_loaders(args=ARGS, train=train, image_transform=image_transform, fold=fold)\n","    #train_loader, valid_loader = prepare_loaders(args=ARGS, train=train, image_transform=image_transform_train, fold=fold)\n","\n","    #vit_l16\n","    #model = create_model(ARGS)\n","    #heavy\n","    model = create_model_timm(ARGS)\n","    model = model.to(ARGS[\"device\"])\n","\n","    #損失関数・最適化関数の定義\n","    optimizer = fetch_optimizer(model.parameters(), optimizer_name=ARGS[\"optimizer\"], lr=ARGS[\"learning_rate\"], betas=(0.9, 0.999))\n","\n","    scheduler = fetch_scheduler(args=ARGS, train_size=len(train_loader), optimizer=optimizer)\n","\n","    model, predictions, targets = one_fold(model, optimizer, scheduler, device=ARGS[\"device\"], num_epochs=ARGS[\"epochs\"], fold=fold)\n","\n","    print(predictions)\n","    train_copy.loc[train_copy[train_copy.kfold == fold].index, \"oof\"] = predictions\n","    #train_copy.loc[train_copy[train_copy.kfold == fold].index, \"pred_0\"] = predictions[:,0]\n","    #train_copy.loc[train_copy[train_copy.kfold == fold].index, \"pred_1\"] = predictions[:,1]\n","\n","    del model, train_loader, valid_loader\n","    _ = gc.collect()\n","    torch.cuda.empty_cache()\n","    print()\n","\n","scores = roc_auc_score(train_copy[\"label\"].values, train_copy[\"oof\"].values)\n","LOGGER.info(f\"========== CV ==========\")\n","LOGGER.info(f\"CV: {scores:.4f}\")"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"CPr04XK_liYf"},"outputs":[],"source":["# OOF\n","#train_copy.to_csv(ARGS['OUT_DIR'] + f'oof.csv', index=False)\n","train_copy.to_csv(f\"{ARGS['OUT_DIR']}/oof_CV{scores:.4f}_1027.csv\", index=False)\n","train_copy.to_csv(f\"{ARGS['DATA_DIR']}/oof_CV{scores:.4f}_1027.csv\", index=False)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"a-K5oJjtlj6l"},"outputs":[],"source":["import os\n","os.environ['oof_CVSroce'] = f\"{scores:.4f}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ig1QcrNVlmUR"},"outputs":[],"source":["!cp -rf ./oof_CV${oof_CVSroce}.csv /content/drive/MyDrive/SIGNATE_TECNO/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VeqrJr1glo53"},"outputs":[],"source":["!ls /content/drive/MyDrive/SIGNATE_TENCO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQ1MpTcxlqbG"},"outputs":[],"source":["#sample_submit.csvを読み込みます\n","#import pandas as pd\n","#submit = pd.read_csv(f\"{ARGS['DATA_DIR']}/sample_submit.csv\", header=None)\n","#submit = pd.read_csv(f\"/content/drive/MyDrive/SIGNATE/Sense/sample_submit.csv\", header=['image_path', 'label'])\n","#submit = pd.read_csv(f\"/content/drive/MyDrive/SIGNATE/Sense/sample_submit.csv\", header=None, names=['image_path', 'label'])\n","test = pd.read_csv(f\"/root/signate_tecno/input/test.csv\", header=None, names=['image_path'])\n","submit = pd.read_csv(f\"/root/signate_tecno/input/sample_submit.csv\", header=None, names=['image_path', 'label'])\n","\n","test.head()\n","#submit.head()"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"NbdzRLMMlthV"},"outputs":[],"source":["# test用のデータ拡張\n","image_transform_test = A.Compose([\n","    A.Resize(ARGS[\"image_size\"][0], ARGS[\"image_size\"][1]),\n","    A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    AT.ToTensorV2 ()\n","    ])\n","test_dataset = CustomDataset(submit, image_transform_test, data_type=\"test\")\n","#test_dataset = CustomDataset2(submit, image_transform_test, data_type=\"test\", is_blurry=ARGS['is_blurry'])\n","test_loader = DataLoader(test_dataset, batch_size=ARGS[\"test_batch_size\"], shuffle=False, num_workers=1) # 4\n","#"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"FEX-PZJslvsI"},"outputs":[],"source":["@torch.no_grad()\n","def valid_fn(model, dataloader, device):\n","    model.eval()\n","\n","    dataset_size = 0\n","    running_loss = 0.0\n","\n","    predict_list = []\n","\n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for step, images in bar:\n","        images = images.to(device)\n","        with torch.no_grad():\n","            outputs = model(images)\n","            #出力にソフトマックス関数を適用\n","            predicts = outputs.softmax(dim=1)\n","\n","        predicts = predicts.cpu().detach().numpy()\n","        predict_list.append(predicts)\n","    predict_list = np.concatenate(predict_list, axis=0)\n","    #予測値が1である確率を提出します。\n","    predict_list = predict_list[:, 1]\n","    gc.collect()\n","\n","    return predict_list"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"bs21fik7lxvM"},"outputs":[],"source":["def inference(model_paths, dataloader, device):\n","    final_preds = []\n","    ARGS['pretrained'] = False\n","    for i, path in enumerate(model_paths):\n","        #model = create_model(ARGS)\n","        model = create_model_timm(ARGS)\n","        model = model.to(device)\n","\n","        #学習済みモデルの読み込み\n","        model.load_state_dict(torch.load(path))\n","        model.eval()\n","\n","        print(f\"Getting predictions for model {i+1}\")\n","        preds = valid_fn(model, dataloader, device)\n","        final_preds.append(preds)\n","\n","    final_preds = np.array(final_preds)\n","    final_preds = np.mean(final_preds, axis=0)\n","    return final_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6ywuPBPlzlQ"},"outputs":[],"source":["#!ls /content/test\n","print(f\"{ARGS['OUT_DIR']}\")\n","!ls ./\n","#!cp -rf ./Score-Fold-0.bin /content/drive/MyDrive/SIGNATE/Sense\n","#!cp -rf ./Score-Fold-1.bin /content/drive/MyDrive/SIGNATE/Sense\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"q3cRS1y2l04v"},"outputs":[],"source":["MODEL_PATHS = [\n","    f\"/root/signate_tecno/output/Score-Fold-{fold}-effnet_lstm.bin\" for fold in range(ARGS[\"n_fold\"])\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bnJT9a4l26H"},"outputs":[],"source":["predict_list = inference(MODEL_PATHS, test_loader, ARGS[\"device\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["count = np.sum((predict_list >= 0.4) & (predict_list < 0.6))\n","print(f\"Count of values in predict_list between 0.4 and 0.6: {count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YkKvA0Nxl4Lj"},"outputs":[],"source":["#submit['label'] = predict_list\n","submit['label'] = (predict_list > 0.5)\n","submit['label'] = submit['label'].astype(int)\n","submit.head()"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["submit.to_csv(f'{ARGS[\"DATA_DIR\"]}/submission_CV_effnet_lstm_1030_cutmix.csv',index = False, header = None)\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["submit[\"predict_proba\"] = predict_list"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submit"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNpSwWm7kv4yVjyhBkgRaJk","gpuType":"A100","machine_shape":"hm","mount_file_id":"140HEFTP_5eKP-RsCeaydYc-clUD8Qp6n","provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2521b23523dd4e3eb778f40df0ea2cf9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34393600ba974eb08b8f9ea86cacea9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6599d195b4c4624b58dbd016998a735","placeholder":"​","style":"IPY_MODEL_f494163acdd34842b4a54a7b4a3be01d","value":" 1.22G/1.22G [02:05&lt;00:00, 12.2MB/s]"}},"647b6ca15c444a3cb3fdaaecfb83dcdf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a05fb91b8a643d38b6305afe8d47268","placeholder":"​","style":"IPY_MODEL_e0cf3e0de68a413a82793eb109c97fc3","value":"model.safetensors: 100%"}},"6a05fb91b8a643d38b6305afe8d47268":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c6811c7c6bf4fdfb7fda09439f0fcb8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6dd16b27d6c4a1d991b761bbba94e82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bf71a57cf16c431b8b81727c6b36ba75":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c6811c7c6bf4fdfb7fda09439f0fcb8","max":1220365908,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6dd16b27d6c4a1d991b761bbba94e82","value":1220365908}},"d69f25abed104e339c23adc51e5ff7da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_647b6ca15c444a3cb3fdaaecfb83dcdf","IPY_MODEL_bf71a57cf16c431b8b81727c6b36ba75","IPY_MODEL_34393600ba974eb08b8f9ea86cacea9b"],"layout":"IPY_MODEL_2521b23523dd4e3eb778f40df0ea2cf9"}},"e0cf3e0de68a413a82793eb109c97fc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f494163acdd34842b4a54a7b4a3be01d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6599d195b4c4624b58dbd016998a735":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
