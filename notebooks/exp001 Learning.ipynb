{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7220,"status":"ok","timestamp":1727844519760,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"ZT929r5200wq","outputId":"ccfa5aa5-6af1-4b6d-e8d6-62c93a5b32c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["32\n"]},{"name":"stderr","output_type":"stream","text":["/root/signate_tecno/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torch.optim as optim\n","import numpy as np\n","from torch.utils.data import DataLoader, Dataset,Subset, random_split\n","import torchvision\n","from torchvision import datasets, models\n","from torchvision import transforms as T\n","import torchvision.transforms.functional as F\n","import torch.nn as nn\n","from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n","import matplotlib.pyplot as plt\n","from IPython.display import display\n","import lightning as L\n","from lightning.pytorch import loggers as pl_loggers\n","from lightning.pytorch.callbacks import ModelCheckpoint\n","import torchmetrics, argparse\n","\n","from torchvision.datasets import ImageFolder\n","\n","from PIL import Image\n","import os\n","\n","import multiprocessing\n","num_workers = multiprocessing.cpu_count()\n","print(num_workers)\n","import timm\n","import wandb"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhayatarou-ay\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1727844519760,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"MmgRv47n75wD"},"outputs":[],"source":["class CFG:\n","    ver = 1.5\n","    seed = 42\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    base_dir =  \"/root/signate_tecno/\"\n","    input_dir = base_dir + \"input/train\"\n","    test_dir = base_dir + \"input/test\"\n","    output_dir = base_dir + \"output/\"\n","    sub_dir  = base_dir + \"submit/\"\n","    log_dir = base_dir + \"logs/\"\n","    model_dir = base_dir + \"model/\"\n","    ckpt_dir = base_dir + \"ckpt/\"\n","\n","    MODEL = \"vit_base\"\n","    DATASET = \"TECNO\"\n","\n","\n","    learning_rate = 1e-3\n","    weight_decay = 1e-5\n","    optimizer = \"AdamW\"\n","    data_aug = \"RandAug\""]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":393,"status":"ok","timestamp":1727757810475,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"W5Bolc_L0-NW","outputId":"a8819525-b87f-43c5-98a7-6af9ea656f1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["['eva02_base_patch14_224.mim_in22k',\n"," 'eva02_base_patch14_448.mim_in22k_ft_in1k',\n"," 'eva02_base_patch14_448.mim_in22k_ft_in22k',\n"," 'eva02_base_patch14_448.mim_in22k_ft_in22k_in1k',\n"," 'eva02_base_patch16_clip_224.merged2b',\n"," 'eva02_enormous_patch14_clip_224.laion2b',\n"," 'eva02_enormous_patch14_clip_224.laion2b_plus',\n"," 'eva02_large_patch14_224.mim_in22k',\n"," 'eva02_large_patch14_224.mim_m38m',\n"," 'eva02_large_patch14_448.mim_in22k_ft_in1k',\n"," 'eva02_large_patch14_448.mim_in22k_ft_in22k',\n"," 'eva02_large_patch14_448.mim_in22k_ft_in22k_in1k',\n"," 'eva02_large_patch14_448.mim_m38m_ft_in1k',\n"," 'eva02_large_patch14_448.mim_m38m_ft_in22k',\n"," 'eva02_large_patch14_448.mim_m38m_ft_in22k_in1k',\n"," 'eva02_large_patch14_clip_224.merged2b',\n"," 'eva02_large_patch14_clip_336.merged2b',\n"," 'eva02_small_patch14_224.mim_in22k',\n"," 'eva02_small_patch14_336.mim_in22k_ft_in1k',\n"," 'eva02_tiny_patch14_224.mim_in22k',\n"," 'eva02_tiny_patch14_336.mim_in22k_ft_in1k',\n"," 'eva_giant_patch14_224.clip_ft_in1k',\n"," 'eva_giant_patch14_336.clip_ft_in1k',\n"," 'eva_giant_patch14_336.m30m_ft_in22k_in1k',\n"," 'eva_giant_patch14_560.m30m_ft_in22k_in1k',\n"," 'eva_giant_patch14_clip_224.laion400m',\n"," 'eva_giant_patch14_clip_224.merged2b',\n"," 'eva_large_patch14_196.in22k_ft_in1k',\n"," 'eva_large_patch14_196.in22k_ft_in22k_in1k',\n"," 'eva_large_patch14_336.in22k_ft_in1k',\n"," 'eva_large_patch14_336.in22k_ft_in22k_in1k']\n"]}],"source":["from pprint import pprint\n","pretrained_models = timm.list_models('eva*',pretrained=True)\n","pprint(pretrained_models)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1727844519760,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"hf28l2JU05qk","outputId":"d5330ae8-13bd-45c7-fd2b-3efbad7502ec"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"markdown","metadata":{"id":"hQygD9oq6SFg"},"source":["## train用のデータセット作成"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":510,"status":"ok","timestamp":1727844921087,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"NO0ClyFV3POe"},"outputs":[],"source":["class ImageDataset(Dataset):\n","    def __init__(self,data_dir,transform=None,phase = \"train\"):\n","        super().__init__()\n","        self.data_dir = data_dir\n","        self.image_paths = []\n","        self.labels = []\n","        for label in os.listdir(data_dir):\n","            label_dir = os.path.join(data_dir, label)\n","            for image_name in os.listdir(label_dir):\n","                image_path = os.path.join(label_dir, image_name)\n","                self.image_paths.append(image_path)\n","                self.labels.append(1 if label == \"hold\" else 0)\n","        self.transform = T.Compose([\n","                                        T.Resize((384,384)),\n","                                        T.RandomHorizontalFlip(p=0.5),\n","                                        T.ToTensor(),\n","                                        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","        self.phase = phase\n","    def __len__(self):\n","        return len(self.image_paths)\n","    def __getitem__(self, index):\n","        image = Image.open(self.image_paths[index])\n","        label = self.labels[index]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":666,"status":"ok","timestamp":1727844923245,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"FzwbvOQC09Fq"},"outputs":[],"source":["class LitDataModule(L.LightningDataModule):\n","    def __init__(self,batch_size = 32, data_dir = \"./input\", ds = None, data_aug = 'RandAug'):\n","        super().__init__()\n","        self.data_dir = data_dir\n","        self.batch_size = batch_size\n","        self.ds = ds\n","\n","    def setup(self,stage = None):\n","        if stage == \"fit\" or stage is None:\n","            num_data = len(self.ds)\n","            print(f'data size: {num_data}')\n","            val_data_size = int(0.2 * num_data)\n","            train_data_size = num_data - val_data_size\n","            self.train_dataset, self.val_dataset = torch.utils.data.random_split(\n","                self.ds, [train_data_size, val_data_size], generator=torch.Generator().manual_seed(42)\n","            )\n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers = num_workers, shuffle=True)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers = num_workers)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12256,"status":"ok","timestamp":1727844935500,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"inGUt3se70Bw","outputId":"d04f8a5a-35ab-469b-cddd-ab3b9a0b98f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["data size: 67446\n"]}],"source":["ds = ImageDataset(data_dir = CFG.input_dir)\n","dm = LitDataModule(data_dir = CFG.input_dir,batch_size = 32,ds = ds)\n","dm.prepare_data()\n","dm.setup(stage = \"fit\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173,"referenced_widgets":["b8048efb787d4f68b52751cb3fbd89ee","1525ebf6b772431ebfad89b582a6fee6","3f3d99fdd66a4951ab8b532e3899fb11","109ae949f93541068fe00c7e83c1e085","844be9c27cde46efb6bcaa5e10648e99","46ffcf7912334f57950a693ed677a0d5","6cd06c25d8d5404699b18cd6d7a839dc","0a452b354c5443b79a9db73a0957c241","b2efd85c08a244f6bcc3d5202832793e","b53eac3f7d32430e9c70c03176cbaac4","933fe9e6e6944787bcf65d304d80886c"]},"executionInfo":{"elapsed":5085,"status":"ok","timestamp":1727844947897,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"iHonsh-3FG6D","outputId":"d110c68d-705d-4cfc-c963-b07305dcd93c"},"outputs":[],"source":["class ViTNet(L.LightningModule):\n","    def __init__(self,learning_rate = 1e-3, weight_decay = 1e-5, optimizer_name = \"SGD\", data_aug = \"RandAug\"):\n","        super().__init__()\n","        self.model = timm.create_model(\"timm/vit_mediumd_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k\" , pretrained = True, num_classes = 2)\n","        # すべての層を固定\n","        for param in self.model.parameters():\n","            param.requires_grad = False\n","\n","        # 'norm', 'fc_norm', 'head_drop', 'head' のみトレーニング可能にする\n","        for param in self.model.norm.parameters():\n","            param.requires_grad = True\n","        for param in self.model.fc_norm.parameters():\n","            param.requires_grad = True\n","        for param in self.model.head_drop.parameters():\n","            param.requires_grad = True\n","        for param in self.model.head.parameters():\n","            param.requires_grad = True\n","        self.learning_rate = learning_rate\n","        self.weight_decay = weight_decay\n","        self.optimizer_name = optimizer_name\n","        self.data_aug = data_aug\n","        self.save_hyperparameters()\n","        self.acc = torchmetrics.classification.Accuracy(task= 'binary')\n","        self.class_acc = torchmetrics.classification.Accuracy(task = 'binary')\n","        self.loss_fn = nn.CrossEntropyLoss()\n","        self.predictions = []\n","        self.training_step_loss = []\n","\n","\n","\n","\n","    def forward(self,x):\n","        out = self.model(x)\n","        return out\n","\n","    def _eval(self,batch,phase, on_step , on_epoch):\n","        x,y = batch\n","        out = self(x)\n","        loss = self.loss_fn(out, y)\n","        preds = torch.argmax(out, dim=1)\n","        acc = self.acc(preds, y)\n","        self.log(f\"{phase}_loss\", loss)\n","        self.log(f\"{phase}_acc\", acc, on_step = on_step, on_epoch = on_epoch)\n","        if phase == \"val\":\n","            self.class_acc(preds,y)\n","            self.log('hp_metric', acc, on_step = False, on_epoch = True,prog_bar = True, logger = True)\n","        return loss\n","\n","    def training_step ( self,batch, batch_idx):\n","        loss = self._eval(batch, \"train\", on_step = False, on_epoch = True)\n","        self.training_step_loss.append(loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        loss = self._eval(batch, \"val\", on_step = False, on_epoch = True)\n","        return loss\n","\n","    def on_train_epoch_end(self) -> None:\n","        all_loss = torch.stack(self.training_step_loss)\n","        self.log(\"train_epoch_loss\", all_loss.mean())\n","\n","\n","    def configure_optimizers(self):\n","        if self.optimizer_name == \"SGD\":\n","            optimizer = optim.SGD(self.parameters(), lr = self.learning_rate, weight_decay = self.weight_decay)\n","        elif self.optimizer_name == \"Adam\":\n","            optimizer = optim.Adam(self.parameters(), lr = self.learning_rate, weight_decay = self.weight_decay)\n","        elif self.optimizer_name == \"AdamW\":\n","            optimizer = optim.AdamW(self.parameters(), lr = self.learning_rate, weight_decay = self.weight_decay)\n","\n","        return optimizer\n","\n","net = ViTNet(learning_rate = CFG.learning_rate,\n","             weight_decay = CFG.weight_decay,\n","             optimizer_name = CFG.optimizer,\n","             data_aug = CFG.data_aug)\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["VisionTransformer(\n","  (patch_embed): PatchEmbed(\n","    (proj): Conv2d(3, 512, kernel_size=(16, 16), stride=(16, 16))\n","    (norm): Identity()\n","  )\n","  (pos_drop): Dropout(p=0.0, inplace=False)\n","  (patch_drop): Identity()\n","  (norm_pre): Identity()\n","  (blocks): Sequential(\n","    (0): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (1): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (2): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (3): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (4): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (5): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (6): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (7): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (8): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (9): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (10): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (11): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (12): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (13): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (14): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (15): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (16): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (17): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (18): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","    (19): Block(\n","      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=512, out_features=512, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","  )\n","  (norm): Identity()\n","  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  (head_drop): Dropout(p=0.0, inplace=False)\n","  (head): Linear(in_features=512, out_features=2, bias=True)\n",")"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["net.model"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1727844947897,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"8Lq2fv-MQG4R"},"outputs":[],"source":["model_checkpoint = ModelCheckpoint(\n","    monitor='val_loss',\n","    dirpath=CFG.ckpt_dir,\n","    filename=f'{CFG.DATASET}-{CFG.ver}-{CFG.MODEL}'+ '-{epoch:02d}-{val_loss:.2f}',\n","    save_top_k=3,\n","    mode='min',\n",")\n","\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',\n","    mode='min',\n","    patience=3,\n",")\n","from pytorch_lightning.loggers import WandbLogger\n","wandb_logger = WandbLogger(name = f'vit_base-{CFG.ver}',save_dir=CFG.log_dir)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1727844947897,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"TIZmdm70Rovt"},"outputs":[],"source":["callbacks = [model_checkpoint, early_stopping]"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":614,"status":"ok","timestamp":1727844956437,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"KsqS2VsLRp0l","outputId":"33083a16-3011-4856-ad11-1002f88e3857"},"outputs":[{"name":"stderr","output_type":"stream","text":["Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]}],"source":["trainer = L.Trainer(default_root_dir=CFG.log_dir,\n","                    max_epochs=10, logger=wandb_logger,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["import torch\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":677,"referenced_widgets":["2b4ec5ec587543028c3abf7fc3ce4899","88a63a2f09ed4ab2878f22047e6b6411","aa540d21d2ec4d73964e98700476d52e","e6d123cc07a74760b8431a95b5e80058","b2ec601f924745b1a39d8dda6427ec57","bff09cc4b06e4d1b9fc8c75d5a24e235","35330d05ffdb41c4b4018407b517158a","207fae1fd649493a963f99821b80e6df","be9ebcf472844ef5aecc96192d473714","a2febbdf582b40f3a130142befea0d7a","afd85e29e43344d497436b12dcee2d23","cb66543f228649e9b194fadf6b76dfaa"]},"id":"6nMtctQsSzuy","outputId":"939f1cce-749b-4659-a53f-3c6770dc8492"},"outputs":[{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/root/signate_tecno/logs/wandb/run-20241003_073710-rkf3ndqs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/hayatarou-ay/lightning_logs/runs/rkf3ndqs' target=\"_blank\">vit_base-1.5</a></strong> to <a href='https://wandb.ai/hayatarou-ay/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/hayatarou-ay/lightning_logs' target=\"_blank\">https://wandb.ai/hayatarou-ay/lightning_logs</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/hayatarou-ay/lightning_logs/runs/rkf3ndqs' target=\"_blank\">https://wandb.ai/hayatarou-ay/lightning_logs/runs/rkf3ndqs</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["data size: 67446\n"]},{"name":"stderr","output_type":"stream","text":["/root/signate_tecno/venv/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:617: Checkpoint directory /root/signate_tecno/ckpt/ exists and is not empty.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name      | Type              | Params\n","------------------------------------------------\n","0 | model     | VisionTransformer | 63.8 M\n","1 | acc       | BinaryAccuracy    | 0     \n","2 | class_acc | BinaryAccuracy    | 0     \n","3 | loss_fn   | CrossEntropyLoss  | 0     \n","------------------------------------------------\n","2.0 K     Trainable params\n","63.8 M    Non-trainable params\n","63.8 M    Total params\n","255.044   Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0: 100%|██████████| 1687/1687 [26:21<00:00,  1.07it/s, v_num=ndqs, hp_metric=0.953]"]},{"ename":"ValueError","evalue":"`self.log(train_epoch_loss, tensor([1.3561, 0.7386, 1.1780,  ..., 0.2353, 0.1040, 0.0250], device='cuda:0'))` was called, but the tensor must have a single element. You can try doing `self.log(train_epoch_loss, tensor([1.3561, 0.7386, 1.1780,  ..., 0.2353, 0.1040, 0.0250], device='cuda:0').mean())`","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:203\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:369\u001b[0m, in \u001b[0;36m_FitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# call train epoch end hooks\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# we always call callback hooks first, but here we need to make an exception for the callbacks that\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# monitor a metric, otherwise they wouldn't be able to monitor a key logged in\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# `LightningModule.on_train_epoch_end`\u001b[39;00m\n\u001b[1;32m    368\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, monitoring_callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 369\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_epoch_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, monitoring_callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    372\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_epoch_end()\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:146\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 146\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    149\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n","Cell \u001b[0;32mIn[9], line 60\u001b[0m, in \u001b[0;36mViTNet.on_train_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     all_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step_loss)\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_epoch_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_loss\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/lightning/pytorch/core/module.py:453\u001b[0m, in \u001b[0;36mLightningModule.log\u001b[0;34m(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/dataloader_idx_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou called `self.log` with the key `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but it should not contain information about `dataloader_idx`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m     )\n\u001b[0;32m--> 453\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumbers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNumber\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__to_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mshould_reset_tensors(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_fx_name):\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;66;03m# if we started a new epoch (running its first batch) the hook name has changed\u001b[39;00m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# reset any tensors for the new hook name\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     results\u001b[38;5;241m.\u001b[39mreset(metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, fx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_fx_name)\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/lightning_utilities/core/apply_func.py:64\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# fast path for the most common cases:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype):  \u001b[38;5;66;03m# single element\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous list\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [function(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n","File \u001b[0;32m~/signate_tecno/venv/lib/python3.8/site-packages/lightning/pytorch/core/module.py:628\u001b[0m, in \u001b[0;36mLightningModule.__to_tensor\u001b[0;34m(self, value, name)\u001b[0m\n\u001b[1;32m    622\u001b[0m value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    623\u001b[0m     value\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor)\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    626\u001b[0m )\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnumel(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.log(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)` was called, but the tensor must have a single element.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You can try doing `self.log(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mean())`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    631\u001b[0m     )\n\u001b[1;32m    632\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n","\u001b[0;31mValueError\u001b[0m: `self.log(train_epoch_loss, tensor([1.3561, 0.7386, 1.1780,  ..., 0.2353, 0.1040, 0.0250], device='cuda:0'))` was called, but the tensor must have a single element. You can try doing `self.log(train_epoch_loss, tensor([1.3561, 0.7386, 1.1780,  ..., 0.2353, 0.1040, 0.0250], device='cuda:0').mean())`"]}],"source":["trainer.fit(net,datamodule = dm)"]},{"cell_type":"markdown","metadata":{"id":"zWY8KGhy6PZS"},"source":["## test用のデータセット作成"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNuqOAsgWNJi"},"outputs":[],"source":["import pandas as pd\n","sample_submit = pd.read_csv(CFG.sub_dir + \"sample_submit.csv\", header=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJJIjZdFo_J0"},"outputs":[],"source":["class TestDatset(Dataset):\n","    def __init__(self,df,data_dir,transform=None):\n","        super().__init__()\n","        self.df = df\n","        self.image_paths = df[0]\n","        self.transform = transform\n","    def __len__(self):\n","        return len(self.df)\n","    def __getitem__(self, index):\n","        image_path = CFG.test_dir +\"/\"+self.image_paths[index]\n","        image = Image.open(image_path)\n","        if self.transform:\n","            image = self.transform(image)\n","        return image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JYELl-xGnVCu"},"outputs":[],"source":["class TestDataModule(L.LightningDataModule):\n","    def __init__(self,df,batch_size = 32, data_dir = \"./input\", ds = None):\n","        super().__init__()\n","        self.df = df\n","        self.data_dir = data_dir\n","        self.batch_size = batch_size\n","        self.test_transform = T.Compose([\n","                                        T.Resize((224,224)),\n","                                        T.ToTensor(),\n","                                        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","    def setup(self,stage = None):\n","        if stage == \"test\" or stage is None:\n","            self.test_dataset = TestDatset(self.df,self.data_dir,self.test_transform)\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers = num_workers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qeX6GjRpVRcB"},"outputs":[],"source":["test_dm = TestDataModule(df = sample_submit,data_dir = CFG.test_dir)\n","test_dm.prepare_data()\n","test_dm.setup(stage = \"test\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1727833484355,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"Kf7tomA1qDsx","outputId":"0ab83138-3c84-46d0-8444-d4494539b41d"},"outputs":[],"source":["test_dm.test_dataloader()"]},{"cell_type":"markdown","metadata":{"id":"euuThofq6mo8"},"source":["## checkpointからモデルをロードして推論"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z7OMzQopjtd1"},"outputs":[],"source":["checkpoint_path = \"/content/drive/MyDrive/SIGNATE_TECNO/ckpt/TECNO-vit-epoch=03-val_loss=0.60.ckpt\"\n","net = ViTNet.load_from_checkpoint(checkpoint_path)\n","net.eval()\n","net.freeze()\n","\n","\n","predict_list, targets_list = [], []\n","\n","for process, images in enumerate(test_dm.test_dataloader()):\n","    images = images.to(CFG.device)\n","    with torch.no_grad():\n","        outputs = net(images)\n","        predicts = outputs.softmax(dim = 1)\n","    predicts = predicts.cpu().detach().numpy()\n","    predict_list.append(predicts)\n","\n","predict_list = np.concatenate(predict_list,axis = 0)\n","\n","predict_list = predict_list[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":115222,"status":"ok","timestamp":1727833015042,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"jnobvaEJsmyM","outputId":"b68f624e-defc-4f19-8566-b6f16966dd59"},"outputs":[],"source":["file_count = len(os.listdir(CFG.test_dir))\n","\n","print(f\"Number of files in {CFG.test_dir}: {file_count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0BILfSRPnI58"},"outputs":[],"source":["sample_submit[1] = predict_list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":471,"status":"ok","timestamp":1727835026344,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"z_Anu7bT_YSP","outputId":"36bec30e-99c1-429a-a6eb-22a66864a2ae"},"outputs":[],"source":["# prompt: predict_listを元に0,1に2値変換してください\n","\n","predicted_labels = (predict_list > 0.5).astype(int)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIpq_dJS_04B"},"outputs":[],"source":["sample_submit[1] = predicted_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":265,"status":"ok","timestamp":1727835110831,"user":{"displayName":"hayatarou shimizu","userId":"02568056446308177595"},"user_tz":-540},"id":"N1dsRd349cVs","outputId":"14af6b68-bf19-455c-fded-2a8f23157d50"},"outputs":[],"source":["sample_submit.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PbXQK4FIc7kD"},"outputs":[],"source":["sample_submit.to_csv(f\"{CFG.sub_dir}/{CFG.ver}-{CFG.MODEL}-{CFG.seed}.csv\", index=False, header=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wpXbaf1c-OB"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMp5DaeCERxNO1yqsU9Qt/q","gpuType":"A100","machine_shape":"hm","mount_file_id":"10FbGmxVe0LKIdwjYlF-svaEJVUzttAMJ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0a452b354c5443b79a9db73a0957c241":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"109ae949f93541068fe00c7e83c1e085":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b53eac3f7d32430e9c70c03176cbaac4","placeholder":"​","style":"IPY_MODEL_933fe9e6e6944787bcf65d304d80886c","value":" 343M/343M [00:01&lt;00:00, 393MB/s]"}},"1525ebf6b772431ebfad89b582a6fee6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46ffcf7912334f57950a693ed677a0d5","placeholder":"​","style":"IPY_MODEL_6cd06c25d8d5404699b18cd6d7a839dc","value":"model.safetensors: 100%"}},"207fae1fd649493a963f99821b80e6df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b4ec5ec587543028c3abf7fc3ce4899":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88a63a2f09ed4ab2878f22047e6b6411","IPY_MODEL_aa540d21d2ec4d73964e98700476d52e","IPY_MODEL_e6d123cc07a74760b8431a95b5e80058"],"layout":"IPY_MODEL_b2ec601f924745b1a39d8dda6427ec57"}},"35330d05ffdb41c4b4018407b517158a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f3d99fdd66a4951ab8b532e3899fb11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a452b354c5443b79a9db73a0957c241","max":343208552,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b2efd85c08a244f6bcc3d5202832793e","value":343208552}},"46ffcf7912334f57950a693ed677a0d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cd06c25d8d5404699b18cd6d7a839dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"844be9c27cde46efb6bcaa5e10648e99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88a63a2f09ed4ab2878f22047e6b6411":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bff09cc4b06e4d1b9fc8c75d5a24e235","placeholder":"​","style":"IPY_MODEL_35330d05ffdb41c4b4018407b517158a","value":"Sanity Checking: "}},"933fe9e6e6944787bcf65d304d80886c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2febbdf582b40f3a130142befea0d7a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa540d21d2ec4d73964e98700476d52e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_207fae1fd649493a963f99821b80e6df","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be9ebcf472844ef5aecc96192d473714","value":0}},"afd85e29e43344d497436b12dcee2d23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2ec601f924745b1a39d8dda6427ec57":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"b2efd85c08a244f6bcc3d5202832793e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b53eac3f7d32430e9c70c03176cbaac4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8048efb787d4f68b52751cb3fbd89ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1525ebf6b772431ebfad89b582a6fee6","IPY_MODEL_3f3d99fdd66a4951ab8b532e3899fb11","IPY_MODEL_109ae949f93541068fe00c7e83c1e085"],"layout":"IPY_MODEL_844be9c27cde46efb6bcaa5e10648e99"}},"be9ebcf472844ef5aecc96192d473714":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bff09cc4b06e4d1b9fc8c75d5a24e235":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6d123cc07a74760b8431a95b5e80058":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2febbdf582b40f3a130142befea0d7a","placeholder":"​","style":"IPY_MODEL_afd85e29e43344d497436b12dcee2d23","value":" 0/? [00:00&lt;?, ?it/s]"}}}}},"nbformat":4,"nbformat_minor":0}
